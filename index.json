
[{"content":" 随便写写。 ","date":"2024-03-16","externalUrl":null,"permalink":"/posts/","section":"博客","summary":" 随便写写。 ","title":"博客","type":"posts"},{"content":"","date":"2024-03-16","externalUrl":null,"permalink":"/","section":"呓语","summary":"","title":"呓语","type":"page"},{"content":"","date":"2024-03-09","externalUrl":null,"permalink":"/tags/dhcp/","section":"Tags","summary":"","title":"DHCP","type":"tags"},{"content":"","date":"2024-03-09","externalUrl":null,"permalink":"/tags/networkmanager/","section":"Tags","summary":"","title":"NetworkManager","type":"tags"},{"content":"NetworkManager （接下来下面将简称 nm） 的 [main] 配置块中，可以选择不同的几种 DHCP Client 实现：\ndhclient dhcpcd internal 配置未指定的情况下默认使用 internal，而前面 2 种都需要额外安装二进制：\nClient Distro / OS family Package dhclient C9S (CentOS Stream 9) dhcp-client dhclient Debian isc-dhcp-client dhcpcd C9S (CentOS Stream 9) dhcpcd dhcpcd Debian 12 dhcpcd dhcpcd Ubuntu 22 dhcpcd5 internal 不依赖任何外部二进制，因为它是 nm 自己内部代码实现的。也就意味着在系统中不会存在由 nm fork 出的其他 DHCP Client 进程。\n但是这个实现的具体情况有一个隐藏的秘密。如果你有幸使用 debug 输出对 nm 进行过调试，就可以在 DHCP 环节通过日志输出发现这个秘密：internal 有两个别名，systemd，或者 libsystemd。\n是的，实际上，这些代码是来自于 systemd，对应 nm 的源代码目录中的 README 也指明了这点。\n以 nm 的 DHCPv6 功能来说，对应 systemd 的代码可以追溯至 sd-dhcp6-client.c\n通过 src/libnm-systemd-shared/README.md 的说明，可以看见 systemd 的对应代码是通过一种比较奇怪的工程实践导入进 nm 的 git 仓库的：从源仓库 clone 后再由脚本复制指定文件。由于我并没有 C++ 工程的实际经验，我也不好评价，但是对此最直观的感受就是：这让对应代码溯源变得较为困难，只能找到对应的 commit内容 才能知道上游 systemd 的代码具体对应关系。\n正如文章最开始所展示的 man page 图中说的那样，internal 的实现不如其他外部实现的 featureful。甚至可以说 systemd 的这些实现还存在很多的改进空间，与现实世界那些林林总总的 DHCP Server 实现稍微有点不兼容，有时会发生一些意想不到的问题。\n比如这个 Issue: DHCPv6 client ignores packets with invalid bytes at the end，描述了在 Oracle 云上的基础设施中，其 VPC 网关实现的 DHCPv6 Reply 消息中尾部含有非法字节，被 systemd 认为是 bad msg，导致 IPv6 的 DHCP 失败。\n比较有意思的是，在 systemd PR 修正后，评论中有人反馈说 Oracle 在最初建议绕过这个问题（让客户不要使用 DHCP）后，在无法知晓的具体时间段悄悄完成了修复。\n我为什么会挖掘并知道这些没用的知识?\n因为我在华为云也碰见了类似的问题。一样是 IPv6 环境下的 DHCP 无法正常获取 IP，在对 nm 开启 debug 后同样看见了对 reply msg 判断为 bad msg 的信息。\n就在我对 nm 的 DHCP 进行了上述情况的粗略挖掘，正准备进行下一步行动时，与同事闲聊了这个意外情况，然后就被其告知了华为云官网的文档链接，其中存在指出在 nm 的 DHCP 配置使用 dhclient 的明确指引段落。尝试了使用 dhclient 后一切正常。\n所以，我个人的建议是，如果你也碰到了 nm 在 DHCP 场景下的问题，可以尝试配置使用 dhclient。\n或者尽量一开始就不用 internal 这个可能存在问题的代码实现，直到 Red Hat 某日在其未来的 C10S，C11S 中将默认的网络管理器设置为 systemd-networkd 的那天。\n","date":"2024-03-09","externalUrl":null,"permalink":"/posts/2024/03/nm-dhcp-implementations-selection/","section":"博客","summary":"NetworkManager （接下来下面将简称 nm） 的 [main] 配置块中，可以选择不同的几种 DHCP Client 实现：","title":"NetworkManager 的 DHCP 选择","type":"posts"},{"content":"","date":"2024-03-09","externalUrl":null,"permalink":"/tags/systemd/","section":"Tags","summary":"","title":"Systemd","type":"tags"},{"content":"","date":"2024-03-09","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2023-06-26","externalUrl":null,"permalink":"/tags/curl/","section":"Tags","summary":"","title":"CURL","type":"tags"},{"content":"偶然看到一篇文章：Go: Calculating public key hashes for public key pinning in curl，其中提到 cURL 的一个有趣的参数：--pinnedpubkey。\n使用过 cURL 的读者可能已经知道 -k / --insecure 参数，在使用 TLS 链接时（如访问 HTTPS 链接）时指定这个参数，将忽略对服务端返回的证书的合法性检查。通常访问使用自签名证书的链接时（典型场景如：非生产环境测试）会使用到它。\n简单来说，TLS 1.3 中的主要流程是：\n通过 ECDH(E) 协议在不可信的信道上进行对称加密密钥及参数的协商 通过 X509 证书进行对端的身份认证（多数情况由链接发起方进行，即客户端检查服务端，双向无非就是下面的流程互相做一次） 证书的合法性判断 1 \u0026ndash; 是否由可信的 CA 签发 证书的拥有者判断 2 \u0026ndash; 是否持有证书中展示的公钥对应的私钥 根据 man page 中的描述，这个 --pinnedpubkey 参数，不需要配合 -k 参数才能使用。个人理解就是 cURL 的一种自定义的、在上述 2 之外的一种额外的身份认证手段。\n--pinnedpubkey 参数的值可以是以下任意一种：\n文件路径，文件的内容需要含有 PEM / DER 格式的公钥 base64 编码的公钥的 sha256 哈希值加上固定的 sha256// 头部，多个值的话需要使用 ; 表示分隔 如果对 OpenSSH 的身份认证较为了解的读者应该会有一种熟悉感，是的，这类身份认证手段在概念上都是近似的。\nOpenSSH 默认将远端主机的 SSH Host Key 的哈希值存储在 ~/.ssh/known_hosts 文件中。\n第二种按哈希方法的具体值怎么计算可以参考文章中的 Go 代码：\nfunc calculatePublicKeyHashes(certs []*x509.Certificate) ([]string, error) { hashes := make([]string, len(certs)) for i, cert := range certs { derCert, err := x509.MarshalPKIXPublicKey(cert.PublicKey) if err != nil { return nil, err } hash := sha256.New() hash.Write(derCert) hashes[i] = fmt.Sprintf(\u0026#34;sha256//%s\u0026#34;, base64.StdEncoding.EncodeToString(hash.Sum(nil))) } return hashes，nil } 忽略代码中这个让人造成困惑的 derCert 变量名 (觉得叫 pkInfoDERBytes 或者直接 bs 更合适)，理解计算的过程就行。\n当然在实际使用中，还是使用文中提出的通过脚本调用 OpenSSL 进行计算的方式更为方便：\n# Assuming default.crt is a PEM-encoded cert, this extracts the public key # converts it to DER form, hashes it with SHA-256, then base64-encodes it # and prepends \u0026#34;sha256//\u0026#34; echo sha256//$(openssl x509 -in default.crt -pubkey -noout \\ | openssl asn1parse -inform PEM -in - -noout -out - \\ | openssl dgst -sha256 -binary - \\ | openssl base64) # Outputs something like sha256//Y/CGGnkaoZwUgOqArQs12llyoaX0bkjSIgHCPtXba+c= 使用公钥的哈希的设计在我看来完全可以直接使用 X509 中的 SKID，只是不知道 cURL 为什么选择了自定义格式的方式。\n这个参数是在 7.39.0 (只支持 OpenSSL) 引入的。见 SSL: implement public key pinning\n类似的 --proxy-pinnedpubkey 在 7.59.0 加入，见 curl: add \u0026ndash;proxy-pinnedpubkey\nRFC 8446 4.4.2. Certificate\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRFC 8446 4.4.3. Certificate Verify\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-06-26","externalUrl":null,"permalink":"/posts/2023/06/curl-x509-pub-key-pinned/","section":"博客","summary":"偶然看到一篇文章：Go: Calculating public key hashes for public key pinning in curl，其中提到 cURL 的一个有趣的参数：--pinnedpubkey。","title":"cURL 的 --pinnedpubkey","type":"posts"},{"content":"","date":"2023-06-26","externalUrl":null,"permalink":"/tags/tls/","section":"Tags","summary":"","title":"TLS","type":"tags"},{"content":"","date":"2023-06-12","externalUrl":null,"permalink":"/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":" SKID 是什么 # 1999 的 RFC 2459 Internet X.509 Public Key Infrastructure Certificate and CRL Profile 中定义了 2 个 X509 v3 证书的扩展概念：\nAuthority Key Identifier，简称 AKI / AKID Subject Key Identifier，简称 SKI / SKID 简单来说，它们是一种标识，对应的意义分别是:\nAKID，用来识别 被特定私钥签发的证书 \u0026ndash; 应使用签发者（issuer）的公钥进行生成 SKID，用来识别 包含特定公钥的证书 \u0026ndash; 应使用证书中的公钥进行生成 所要确保的就是，证书 Y 中的 AKID，必然是签发证书 Y 的 CA 证书 X 的 SKID。\n以上图 1 的证书路径/链条构造为例，Root CA 证书 1 的 SKID 就是 CA 证书 2 的 AKID，而 CA 证书 2 的 SKID 又是 CA 证书 3 中的 AKID。\nRFC 明确要求所有被 CA 签发的下级证书都必须存在 AKID，只有 \u0026ldquo;自签发\u0026rdquo; 的证书才可以作为特例而省略填充这个字段。而 SKID 则是要求必须出现在 CA 证书中。\n这两个扩展设计出来的意义主要是为了更方便的进行证书路径/链条的构建，通俗点说的话，就是给定一个被签发的证书 X，可以通过其 AKID 快速的在 CA Pool 中找到签发它的 \u0026ldquo;Parent\u0026quot;。\n这只是用来进行对象的快速查找，具体找到的关联对象是不是真的 \u0026ldquo;Parent\u0026rdquo; 还需要进行校验签名的判断。\n想要用 SKID 查询证书的话可以去这里：crt.sh\n具体 SKID 应该如何生成，标准中定义了两种 \u0026ldquo;常用方法\u0026rdquo;：\nOpenSSL 在这个对文档进行补充的 提交 中专门说明了生成 SKID 的实现方法是按照 RFC 5280 的 4.2.1.2. (1)，其实也就是 RFC 2459 的 4.2.1.2. (1). 因为虽然 RFC 2459 逐步被新发布的 3280、5280 所淘汰，但是新 RFC 中关于 AKID、SKID 的这部分没什么变更。\n虽然没有去看具体的 C 代码，但我想既然是专门在文档补充这个细节，大概率表示 OpenSSL 应该一直都是使用这个方法进行 SKID 的生成。\n其实标准中在 \u0026ldquo;常用方法\u0026rdquo; 的定义下紧接着说到 Other methods of generating unique numbers are also acceptable.，但我不禁怀疑是否真的有使用自定义方法进行实现的软件。直到我偶然了解到 Go 中 SKID 的故事。\nGo 中的故事 # 在 Golang 1.15 之前，标准库 x509 中的 CreateCertificate 方法不会对 CA 类型的证书自动的添加上 SKID，这个行为明显是违反了 RFC 的要求。这个情况是在 2018 年被发现的：crypto/X509: add SubjectKeyId automatically when IsCA is true。按这个 issue 中的讨论看，Go 团队认为将其在 1.14 中修复是赶不上了，于是直到 2020 年 4 月才有了修复的提交。\n然而，这个提交的代码实现是有点。。。 特别的。 作者选择直接将 SPKI 整个结构体 marshal 出的 bytes 进行 hash，并不是上面章节中提到的 RFC 定义的两种常用方法的任意一种。\n虽然按 RFC 的意思，你搞自定义的也不是不可以。但要真按这个思路生成出来的证书，在与其他使用 RFC 的「常用方法」进行 SKID 生成的实现（比如 OpenSSL）进行交互时就很难说会不会产生意料之外的兼容性问题。\n好在这个情况仅仅过了 2 个月就被发现：crypto/X509: SKID generation is over full spki，rather than just the subjectPublicKey，也赶在 1.15 正式发布前完成了修复。\n可以看见最终是选择了第一种方法，对公钥的 bytes 做 SHA-1 的 HASH。\n吐槽 # 比较难以想象，这是一个 2020 年 6 月 9 号才修复的问题，并且正式的 1.15 版本是 2020 年 8 月 11 号才发布的。\nRFC 2459 是 1999 年的东西了。就算以 2008 年的 RFC 5280 做标准，在随后漫长的互联网产业扩张并「造福世界」的过程中，各大证书厂商/开源工具等对于 SKID 这个东西的操作可以说应该是已经相当成熟。\n虽然 Golang 是一个比较年轻的语言，这也只是实现 RFC 标准的过程中出现的小瑕疵，不是什么很致命/关键的错误，但联想到之前的 OpenSSL Heartbleed 事件，是真的越是底层的基础设施就越是这样得不到该有的关注和投入，还是我太想当然了呢?\nAKI and SKI Extension\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-06-12","externalUrl":null,"permalink":"/posts/2023/06/golang-x509-skid/","section":"博客","summary":"SKID 是什么 # 1999 的 RFC 2459 Internet X.","title":"Go X.509 标准库中 SKID 的故事","type":"posts"},{"content":"","date":"2023-06-12","externalUrl":null,"permalink":"/tags/x509/","section":"Tags","summary":"","title":"X509","type":"tags"},{"content":"","date":"2023-06-03","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"2023-06-03","externalUrl":null,"permalink":"/categories/cryptography/","section":"Categories","summary":"","title":"Cryptography","type":"categories"},{"content":"","date":"2023-06-03","externalUrl":null,"permalink":"/tags/ecc/","section":"Tags","summary":"","title":"ECC","type":"tags"},{"content":"最近由于工作原因，接触了不少现代计算机密码学知识。想写点东西分享分享，但由于学的都是些皮毛，属于将就能够应用的水平，这里只能写一篇杂谈，闲聊下 ECC 和 25519。\nECC 简介 # 现代计算机的安全性离不开公钥密码学，而公钥密码学的基础构建在数学之上。与较为出名的 RSA 加密系统的算法建立在分解两个大素数乘积（因式分解）的困难性上不同，椭圆曲线密码学 Elliptic-curve cryptography，即 ECC，是建立在求解椭圆曲线离散对数问题的困难度上。其显著的特点是，对比 RSA 来说 ECC 实现同等加密强度所需的密钥长度要小很多，并且所需的算力开销也更小。\n基于 ECC 的具体应用主要分布在两方面：\n数字签名 \u0026ndash; 被广泛应用在身份认证、完整性检查等领域 密钥协商 \u0026ndash; 用于现代加密通信最关键的部分：在不可信的信道上进行加密密钥的协商 而目前世界上较为流行的 ECC 标准又可大致分为两类：\n由 NIST (美国国家标准与技术研究院) 推荐的多个曲线，定义在 2000 年发布的 FIPS 186-2 中 由世界著名的密码学家 Daniel J. Bernstein 在 2006 年发布的 25519 曲线 由于较早推出，NIST 的曲线得到了广泛的应用。如数字签名算法 ECDSA 在 X509 证书标准中的使用，生产出的对象即是现如今正逐步普及的 ECC 证书。\n而在像 OpenSSL 等一众开源 TLS 实现中，NIST 的曲线在密钥协商 ECDH 中更是被作为优先选择。\n在很长的一段时间内，NIST Curve P-256 是 OpenSSL ECDH 默认曲线列表中的首选。\nOpenSSL 代码中虽然没有注释说明是 NIST 标准，我们可以自行对 P-256 的 P 简单做个换算看看 Hex 是否一致。\n可以发现确实是 NIST 的曲线无疑。\n可疑的 NIST 椭圆曲线，NSA 的后门劣迹 # 从上面 OpenSSL 的代码截图中可以看见，NIST 的 P-256 曲线被叫做 PRIME_256V1. 这个名字其实来自于 1998 年的 ANSI X9.62 ECDSA 规范，至于为什么 NIST 的曲线参数跟这个 ANSI 规范的一模一样，是因为后来的 NIST 直接使用了这些曲线。那么问题是，这些曲线为何这样定义？它们又是哪里来的？这两个问题至今没有清晰的答案。因为现实就是这些曲线中的 Seed 定义既没有解释或理由，也没有任何连来历信息。但是在各种密码学、计算机圈子内都倾向于一个统一的，合理的猜测 \u0026ndash; 它们是来自于 NSA（美国国家安全局）。\n那么，更为重要的灵魂问题是: 它们是否存在弱点或后门？\n如果说从这些曲线发布的那天起，对其的质疑 1 只在小范围专业圈子内传播，到了 2013 年 9 月 6 日在纽约时报网络版上的一篇新闻 2 披露出的内容则是在公共领域掀起了轩然大波：根据 爱德华·斯诺登（对，就是棱镜门的主角）披露的机密文件显示，NSA 曾经向某密码学算法植入后门并将其通过 NIST 确立为国家标准。虽然文内并未直接披露是什么算法，但却隐晦的描述了这样的一段话：\nClassified N.S.A. memos appear to confirm that the fatal weakness, discovered by two Microsoft cryptographers in 2007, was engineered by the agency. The N.S.A. wrote the standard and aggressively pushed it on the international group, privately calling the effort \u0026ldquo;a challenge in finesse.\u0026rdquo;\n这让业界很快就锁定到了具体的对象，就是早在 2007 年就由两名来自微软的密码学研究专家发现可以被植入后门 3 的伪随机数生成算法：来自 NIST SP800-90 的 Dual_EC_DRBG.\n紧接着在同年的 12 月，路透社更是直接独家爆料出了 NSA 对 RSA 公司进行 $10 million 的贿赂，将 Dual_EC_DRBG 作为 RSA 商业产品 Bsafe 中首选的随机数生成算法的事 4。\n很快，越来越多的证据都指出 Dual_EC_DRBG 确实存在可能的后门 5 6，工业界纷纷开始抛弃 Dual_EC_DRBG，并且随之而来的还有愈演愈烈的对 NIST 曲线的质疑 7 8 9，可以说从 2013 后，密码学进入了 「后斯诺登时代」，业界急需一个透明、公开、可审查、可持续改进的 ECC 实现来摆脱 NSA 这个\u0026quot;美国老大哥\u0026quot;给出的可疑曲线。\nCurve25519，X25519，Ed25519 # 随着业界对 NIST 椭圆曲线的不信任加剧，原先于 2006 年发布后只在学术界传播的 25519 曲线热度迅速提升，各工业界的软件开始纷纷加入对 25519 曲线的支持。一方面是信任问题，另一方面是 ECC 在工业界使用上存在可能的专利纠纷 10，而 Curve25519 是公共领域的产物，不受任何专利保护 11。\nRFC 在 2016 年 1 月 推出的 RFC 7748 明确了在 ECDH 协议中应用 25519 曲线的 X25519 方法，而 Ed25519 则是出现在 2017 年 1 月的 RFC 8032 这个对 EdDSA 数字签名算法进行了更加明晰的标准化规范中，可以说自此之后，可疑的 NIST 曲线将会逐步的退出历史舞台。\n从理论上说，25519 曲线的优势在于：\n没有专利约束\n公开，透明的参数，可信度高 \u0026ndash; 不像 NIST 的曲线那样存在可疑的、来路不明的 Seed\n实用安全性高，力图通过精心设计的实用性做好安全性防护\n理论上的安全性不等于实际上的安全性，因为不恰当的实现导致的漏洞不在少数 \u0026ndash; 最著名的例子就是索尼的 PS3 12 13\n对于曲线的安全性探究可以参考：SafeCurves: choosing safe curves for elliptic-curve cryptography\n性能优异，计算速度上比 NIST 曲线要快很多，对低算力的硬件更友好\n公钥长度很短，只有 32 bytes\n按 Go 标准库中给出的算法计算了下，NIST 的各个曲线的公钥长度如下：\nNIST Curve Public Key Bytes Length P-224 57 P-256 65 P-384 97 P-521 133 可以看出 25519 这个公钥长度确实比 NIST 的要短很多。\nIANIX 专门有一个网页列出了使用 25519 曲线的软件/软件库：Things that use Curve25519，感兴趣的读者可以自行浏览。\n下面简单聊聊几个我个人接触较多的使用了 25519 曲线的领域/软件的情况。\nOpenSSH 中的 25519 # SSH 的密钥协商没有看见使用 ECDH 这个术语，而是直接叫 KEX，即 Key Exchange.\nOpenSSH 可以通过 ssh -Q kex 查看支持的 KEX 算法\n想要知道其他不同的 SSH 实现对 KEX 的支持，可以参考这个矩阵：Key exchange protocols\nLibSSH 的 Aris Adamantiadis 于 2013 年 9 月 27 日为 LibSSH 的 KEX 添加了 25591 曲线的支持: kex: implement curve25519-sha256@libssh.org：\n是的，就在纽约时报那篇报道之后的同月就完成了，这可能是世界上第一个实现应用 25519 曲线的 SSH 功能库。引用他自己的描述 14 是：\nThis algorithm does not rely on NIST-based curves and gives us more security confidence against a possible backdoor in nistp-256 curve.\n随后这个功能作为 Patch 15 被他提交给了 OpenBSD 团队，并且在 2013 年 11 月 3 日进入 OpenBSD 的主线 use curve25519 for default key exchange (curve25519-sha256@libssh.org);：\n密钥协商的功能有了，剩下的身份认证那块（Host Key 和 User Authentication Key）对 Ed25519 的支持也在同年的 12 月 完成 support ed25519 keys (hostkeys and user identities) using the public …：\nOpenSSH 可以通过 ssh -Q key 查看支持的 Host / User Authentication Key 列表\n想要知道其他不同的 SSH 实现对 Host / User Authentication Key 的支持，可以参考这个矩阵: Hostkey formats\n所有上述的特性都随着 2014 年 1 月 30 日发布的 OpenSSH 6.5 到来：\nOpenBSD 的发布在 5.5，晚了几个月。\n上面的两类特性分别在 2020 年的 RFC 8709 和 RFC 8731 被标准化。8709 描述的是如何在 SSH 协议中使用 EdDSA ，8731 则是 KEX 中对 25519 和 448 的使用。\n但是我看现在 OpenSSH 也没支持 448 曲线 16。\nTLS 中的 25519 # 从 IANIX 给出的时间线看，在 RFC 7748 正式发布之前，很多 TLS 相关的软件已经根据 RFC 草案等实现了对 25519 的支持。\n看得出搜索巨人 Google 的动作还算比较迅速，毕竟这是出于 Chromium / Chrome 的现实需求。\n从发布声明上看，LibreSSL 的实现（2017-02-01，2.5.1）比它那个兄弟 OpenSSL 还要晚接近半年。虽然这可能是因为 LibreSSL 要从 OpenBSD 扣代码导致的，但实际上 OpenBSD 仓库中的实现就是比 OpenSSL 要更晚 17。\nLibsodium 更晚，在 2017 年 3 月 的 1.0.12 才支持\nGolang 中 TLS 标准库对 X25519 的支持是在 2017 年 2 月发布的 1.8 引入的，见 crypto/tls: support X25519 key exchange。比较神奇的是，从这个 issue 中甚至能发现 Google 内部在 2015 年 12 月对 BoringSSL 做测试时使用的是 Go 代码，即当时就已做到用 Go 实现了 X25519。\nX509 中的 25519 # 2018 年 8 月发布的 RFC 8410 中正式将 EdDSA 运用在 X509 中，同时也对 RFC 7748 中定义的 X25519 补充了下 ASN.1 的 OID 等信息。可能读者会奇怪：X25519 不是 ECDH 协议使用的算法吗，怎么会有 ASN.1 的定义？原因在于这个 RFC 标准中还有一个比较奇怪的用法: 将 X25519 的公钥签名后放在证书的结构中，所以才需要定义 X25519 的 OID。不过我也没太搞明白具体的用法和意义，网上的资料也相对较少 18。\nOpenSSL 倒是在同年 9 月 20 日发布的 1.1.1 正式版 19 20 中支持了 EdDSA 签名的证书，但是截止目前（文章编写时），现实世界中的可信 CA 组织普遍都不支持签发使用 Ed25519 签名方案的证书。\n在 Google Groups 上的 sci.crypt Usenet 讨论存档 NIST annouces set of Elliptic Curves\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN.S.A. Able to Foil Basic Safeguards of Privacy on Web\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOn the Possibility of a Back Door in the NIST SP800-90 Dual Ec Prng\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExclusive: Secret contract tied NSA and security industry pioneer\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDual EC in X9.82 and SP 800-90\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYouTube 视频 How did the NSA hack our emails?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhy I don’t Trust NIST P-256\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA+ Fail\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAn overview of the new features in GnuTLS 3.5.0\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nECC patents\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIrrelevant patents on elliptic-curve cryptography\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nConsole Hacking 2010 - PS3 Epic Fail\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHackers Describe PS3 Security As Epic Fail, Gain Unrestricted Access\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenSSH introduces curve25519-sha256@libssh.org key exchange !\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n[PATCH] curve25519-sha256 at libssh.org key exchange proposal\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenSSH Specifications\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAdd support for ECDHE with X25519.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ncryptography 项目中的 issue，评论中描述了可能的 X25519 证书用途 X25519 certificates\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPR #3503 Ed25519 support\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAdd Ed25519 algorithm.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-06-03","externalUrl":null,"permalink":"/posts/2023/06/ecc-meandering/","section":"博客","summary":"最近由于工作原因，接触了不少现代计算机密码学知识。想写点东西分享分享，但由于学的都是些皮毛，属于将就能够应用的水平，这里只能写一篇杂谈，闲聊下 ECC 和 25519。","title":"ECC 杂谈：NIST / NSA 后门，Curve25519","type":"posts"},{"content":"","date":"2022-11-10","externalUrl":null,"permalink":"/tags/dns/","section":"Tags","summary":"","title":"DNS","type":"tags"},{"content":"","date":"2022-11-10","externalUrl":null,"permalink":"/tags/macos/","section":"Tags","summary":"","title":"MacOS","type":"tags"},{"content":"要实现标题所描述的目的，在 macOS 上有两种手段。\n本文所述命令操作均使用 macOS Ventura 13.0 完成。\nBSD style，resolver file # 从很古老的 OS X 时期起，mac 上就支持通过配置 resolver 文件的方式来设置 DNS。文件的具体格式可以通过 man 5 resolver 了解。\n具体用法可以简要概述为，只需要在 /etc/resolver 文件夹下（需要自行创建这个文件夹，默认不存在）下创建一个含有 resolver 格式内容的文件，比如 /etc/resolver/example.com，就能让 example.com 的域名解析都使用这个文件内指定的 DNS 服务器地址。不再需要的话只要删除掉对应文件即可。\n以我家中网络使用的 lan 这个 TLD 为例，文件最简单的内容只需要写成这样：\ndomain lan search lan nameserver 192.168.15.1 创建或修改了这个文件后执行 scutil --dns 查看是否出现了对应的 resolver：\n$ scutil --dns DNS configuration resolver #1 nameserver[0] : fe80::800c:67ff:fe1d:fd64%12d nameserver[1] : 172.20.10.1 if_index : 12 (en1) flags : Request A records, Request AAAA records reach : 0x00020002 (Reachable,Directly Reachable Address) resolver #2 domain : local options : mdns timeout : 5 flags : Request A records, Request AAAA records reach : 0x00000000 (Not Reachable) order : 300000 resolver #3 domain : 254.169.in-addr.arpa options : mdns timeout : 5 flags : Request A records, Request AAAA records reach : 0x00000000 (Not Reachable) order : 300200 resolver #4 domain : 8.e.f.ip6.arpa options : mdns timeout : 5 flags : Request A records, Request AAAA records reach : 0x00000000 (Not Reachable) order : 300400 resolver #5 domain : 9.e.f.ip6.arpa options : mdns timeout : 5 flags : Request A records, Request AAAA records reach : 0x00000000 (Not Reachable) order : 300600 resolver #6 domain : a.e.f.ip6.arpa options : mdns timeout : 5 flags : Request A records, Request AAAA records reach : 0x00000000 (Not Reachable) order : 300800 resolver #7 domain : b.e.f.ip6.arpa options : mdns timeout : 5 flags : Request A records, Request AAAA records reach : 0x00000000 (Not Reachable) order : 301000 resolver #8 domain : lan search domain[0] : lan nameserver[0] : 192.168.15.1 flags : Request A records, Request AAAA records reach : 0x00000002 (Reachable) DNS configuration (for scoped queries) resolver #1 nameserver[0] : fe80::800c:67ff:fe1d:fd64%12d nameserver[1] : 172.20.10.1 if_index : 12 (en1) flags : Scoped, Request A records, Request AAAA records reach : 0x00020002 (Reachable,Directly Reachable Address) 可以看见 #8，表明配置已经生效。\n这个 scutil 命令将在下一章描述。\n如果是 macOS 10.10.4+ 的系统，在创建或修改了这个文件后，建议给 mDNSResponder 发送 HUP 信号来清除系统的 DNS 缓存。\nkillall -HUP mDNSResponder 测试配置是否生效可以使用 dns-sd 命令：\n$ dns-sd -q netbox DATE: ---Wed 09 Nov 2022--- 23:21:40.760 ...STARTING... Timestamp A/R Flags IF Name Type Class Rdata 23:21:40.836 Add 3 0 netbox.lan. CNAME IN unicorn.lan. 23:21:40.836 Add 2 0 unicorn.lan. Addr IN 192.168.15.2 ^C 也可以在 -G 后指定 v4 / v6 来查询 A / AAAA：\n$ dns-sd -G v4v6 netbox DATE: ---Wed 09 Nov 2022--- 23:23:41.669 ...STARTING... Timestamp A/R Flags IF Hostname Address TTL 23:23:41.686 Add 2 0 unicorn.lan. 192.168.15.2 15 23:23:41.700 Add 2 0 netbox. 0000:0000:0000:0000:0000:0000:0000:0000%\u0026lt;0\u0026gt; 77 No Such Record ^C System Configuration Framework，scutil # 在 macOS 上，还有一套称为 System Configuration Framework 的高级系统框架，最早可能可以追述到 Mac OS X Leopard 时代 1。这套框架用于动态管理系统中的各个配置项，当然也包括配置系统 DNS 的能力。根据官方文档 System Configuration 页面上的描述，它基本支持了 Apple 所有的系统，如 iOS / iPadOS / macOS 等等。像 iOS 上的 APP 可以通过调用这套框架对应的系统 API SCDynamicStore 来设置 DNS，而对 macOS 上的管理员 / 用户来说，更实用的手段是通过上一章提到过的 scutil 命令。\nsc 是 System Configuration 首字母的缩写。\n通过查看 scutil 的 man page，可以在 --dns 这个选项下的描述中发现，系统上的 DNS 解析库依然保持着对旧有的 BSD 风格 resolver 文件的兼容和适配：\n结合上一章节中的实际表现也确实如此。只是按照 Apple 的风格，突然未来的某一天系统更新就不支持这个特性了也说不准，所以提前适应新工具也是有一定必要的。下面就展示下如何使用 scutil 进行 DNS 的配置。\nscutil 执行指令的方法是交互式的，执行它会进入一个类似 shell 的场景。但是你也可以像调用其他普通命令那样来调用它，比如使用管道方法传递 stdin：\n$ echo help | scutil Available commands: help : list available commands f.read file : process commands from file quit : quit d.init : initialize (empty) dictionary d.show : show dictionary contents d.add key [*#?%] val [v2 ...] : add information to dictionary (*=array，#=number，?=boolean，%=hex data) d.remove key : remove key from dictionary list [pattern] : list keys in data store add key [\u0026#34;temporary\u0026#34;] : add key in data store w/current dict get key : get dict from data store w/key set key : set key in data store w/current dict show key [\u0026#34;pattern\u0026#34;] : show values in data store w/key remove key : remove key from data store notify key : notify key in data store n.list [\u0026#34;pattern\u0026#34;] : list notification keys n.add key [\u0026#34;pattern\u0026#34;] : add notification key n.remove key [\u0026#34;pattern\u0026#34;] : remove notification key n.changes : list changed keys n.watch : watch for changes n.cancel : cancel notification requests 或者使用 heredoc：\n$ scutil \u0026lt;\u0026lt; EOF heredoc\u0026gt; help heredoc\u0026gt; EOF Available commands: help : list available commands f.read file : process commands from file quit : quit d.init : initialize (empty) dictionary d.show : show dictionary contents d.add key [*#?%] val [v2 ...] : add information to dictionary (*=array，#=number，?=boolean，%=hex data) d.remove key : remove key from dictionary list [pattern] : list keys in data store add key [\u0026#34;temporary\u0026#34;] : add key in data store w/current dict get key : get dict from data store w/key set key : set key in data store w/current dict show key [\u0026#34;pattern\u0026#34;] : show values in data store w/key remove key : remove key from data store notify key : notify key in data store n.list [\u0026#34;pattern\u0026#34;] : list notification keys n.add key [\u0026#34;pattern\u0026#34;] : add notification key n.remove key [\u0026#34;pattern\u0026#34;] : remove notification key n.changes : list changed keys n.watch : watch for changes n.cancel : cancel notification requests 如果编写脚本，建议使用 help 中提到的 f.read 直接从文件中读取指令，或者使用 heredoc 的方法。这两种方法在连续执行多个指令的情况下比管道传递 stdin 能够提供更好的阅读性。\n管道传递 stdin 方法中多个指令之间需要包含 \\n。\n具体如何配置 DNS，scutil 的文档并未做出过多解释，毕竟它也只是调用的 SystemConfiguration.framework SCDynamicStore APIs 去设置 K / V，但让人无语的是，框架的文档页只列出了配置 DNS 的 Key：DNS Entity Keys，具体这些 Key 代表什么，每个 Key 值的格式等也并未做出详细说明。\n好在有 Stack Exchange 上的 mecki 的回答 2 3，给出了使用 scutil 配置的完整例子：\n#!/bin/bash sudo scutil \u0026lt;\u0026lt; EOF d.init d.add ServerAddresses * 9.9.9.9 d.add SearchDomains * stackexchange.com d.add SupplementalMatchDomains * stackexchange.com set State:/Network/Service/whatever-you-want-as-long-as-unique/DNS exit EOF 注意 SearchDomains 这行，这个 key 在 mecki 的回答中没有提及，是我自己对照 DNS Entity Keys 后测试的，实际结果显示它没什么用。\n对于 mecki 怎么知道使用什么 Key 这回事，从他能给出 Apple 这些组件源码在 GitHub 的地址 这情况看，十有八九是他看过代码。\n语法中的 * 是一个特殊标识，用来表示后面值是 Array 类型。\nd.add SearchDomains stackexchange.com 创建出的对象：\n\u0026gt; get State:/Network/Service/whatever-you-want-as-long-as-unique/DNS \u0026gt; d.show \u0026lt;dictionary\u0026gt; { SearchDomains : stackexchange.com ServerAddresses : \u0026lt;array\u0026gt; { 0 : 9.9.9.9 } SupplementalMatchDomains : \u0026lt;array\u0026gt; { 0 : stackexchange.com } } d.add SearchDomains * stackexchange.com 创建出的对象：\n\u0026gt; get State:/Network/Service/whatever-you-want-as-long-as-unique/DNS \u0026gt; d.show \u0026lt;dictionary\u0026gt; { SearchDomains : \u0026lt;array\u0026gt; { 0 : stackexchange.com } ServerAddresses : \u0026lt;array\u0026gt; { 0 : 9.9.9.9 } SupplementalMatchDomains : \u0026lt;array\u0026gt; { 0 : stackexchange.com } } 配置完后还是老样子，通过 scutil --dns 查看是否出现了对应的 resolver：\n$ scutil --dns DNS configuration resolver #1 search domain[0] : stackexchange.com search domain[1] : lan nameserver[0] : 192.168.15.1 nameserver[1] : fd6f:9316:1db::1 if_index : 12 (en1) flags : Request A records, Request AAAA records reach : 0x00020002 (Reachable,Directly Reachable Address) resolver #2 domain : stackexchange.com nameserver[0] : 9.9.9.9 flags : Supplemental, Request A records, Request AAAA records reach : 0x00000002 (Reachable) order : 101800 ... 可以看出明显的与 resolver file 配置时不一样的输出。\n使用 dns-sd 测试：\n$ dns-sd -q apple DATE: ---Thu 10 Nov 2022--- 00:13:03.209 ...STARTING... Timestamp A/R Flags IF Name Type Class Rdata 00:13:03.211 Add 40000003 0 apple.stackexchange.com. Addr IN 172.64.144.30 00:13:03.211 Add 40000002 0 apple.stackexchange.com. Addr IN 104.18.43.226 ^C 如果想要移除这个配置只需删除这个 Key 即可：\nsudo scutil remove State:/Network/Service/whatever-you-want-as-long-as-unique/DNS 要在 scutil 中查看还有其他哪些 DNS 的 Keys 可以使用 list \u0026quot;.*DNS\u0026quot;：\n$ echo \u0026#39;list \u0026#34;.*DNS\u0026#34;\u0026#39; | scutil subKey [0] = State:/Network/Global/DNS subKey [1] = State:/Network/MulticastDNS subKey [2] = State:/Network/PrivateDNS subKey [3] = State:/Network/Service/C6BA5C2B-B3FB-4A34-99EA-EE62154A2BD6/DNS subKey [4] = State:/Network/Service/whatever-you-want-as-long-as-unique/DNS 其他使用 scutil 的例子可以参考：\nVPN client over-riding DNS on macOS How to add some additional DNS search domains without ignoring the DHCP ones? Tinc integration # 如果有读者像我一样使用 tinc，可以参考一下我简单的 up / down 脚本。\nresolver file # /opt/homebrew/etc/tinc/z10n0110/tinc-up:\n#!/bin/sh ifconfig $INTERFACE 10.0.7.3 10.0.7.9 netmask 255.255.0.0 route add -net 10.0.7.0/24 -interface $INTERFACE route add -net 192.168.15.0/24 -interface $INTERFACE mkdir -p /etc/resolver cat \u0026gt; /etc/resolver/lan \u0026lt;\u0026lt; HERE domain lan search lan nameserver 192.168.15.1 HERE cat \u0026gt; /etc/resolver/z10n0110.men \u0026lt;\u0026lt; HERE domain z10n0110.men search z10n0110.men nameserver 192.168.15.1 HERE killall -HUP mDNSResponder /opt/homebrew/etc/tinc/z10n0110/tinc-down:\n#!/bin/sh route delete -net 192.168.15.0/24 -interface $INTERFACE route delete -net 10.0.7.0/24 -interface $INTERFACE ifconfig $INTERFACE down if [ -f /etc/resolver/lan ]; then rm -rf /etc/resolver/lan fi if [ -f /etc/resolver/z10n0110.men ]; then rm -rf /etc/resolver/z10n0110.men fi killall -HUP mDNSResponder scutil # /opt/homebrew/etc/tinc/z10n0110/tinc-up:\n#!/bin/sh ifconfig $INTERFACE 10.0.7.3 10.0.7.9 netmask 255.255.0.0 route add -net 10.0.7.0/24 -interface $INTERFACE route add -net 192.168.15.0/24 -interface $INTERFACE cat \u0026gt; /opt/homebrew/etc/tinc/z10n0110/up-cmds.scutil \u0026lt;\u0026lt; HERE d.init d.add ServerAddresses * 192.168.15.1 d.add SupplementalMatchDomains * lan z10n0110.men set State:/Network/Service/tinc-z10n0110/DNS exit HERE echo \u0026#39;f.read /opt/homebrew/etc/tinc/z10n0110/up-cmds.scutil\u0026#39; | scutil killall -HUP mDNSResponder /opt/homebrew/etc/tinc/z10n0110/tinc-down:\n#!/bin/sh route delete -net 192.168.15.0/24 -interface $INTERFACE route delete -net 10.0.7.0/24 -interface $INTERFACE ifconfig $INTERFACE down echo \u0026#39;remove State:/Network/Service/tinc-z10n0110/DNS\u0026#39; | scutil killall -HUP mDNSResponder 2009 年的文章：Overriding DHCP- or VPN-assigned DNS servers in Mac OS X Leopard\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDo /etc/resolver/ files work in Mountain Lion for DNS resolution?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhat\u0026rsquo;s the grammar of the Dynamic Store\u0026rsquo;s DNS config?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-11-10","externalUrl":null,"permalink":"/posts/2022/11/set-specific-dns-nameservers-for-specific-domain-on-macos/","section":"博客","summary":"要实现标题所描述的目的，在 macOS 上有两种手段。","title":"macOS 下让特定域名的解析使用指定的 DNS Nameserver","type":"posts"},{"content":"","date":"2022-11-08","externalUrl":null,"permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"Prometheus","type":"tags"},{"content":"最近给 Grafana 的 Node Exporter Full 这个公开仪表盘提了一个 PR. 主要是解决与新版的 Grafana 9 的兼容问题。\n这个仪表盘中的 CPU 面板图中，需要按顺序精心配置 6 个相差不大的对应每个 CPU 模式的查询。为什么说相差不大，因为其实都是在查同一个指标：node_cpu_seconds_total，只是 mode 不同而已。这样做的目的是需要保证 mode=\u0026quot;idle\u0026quot; 这个查询的展示排在最后，因为这个面板的 stack series 渲染逻辑需要如此。今天突然想到是否能只配置一次查询来做到一样的效果，只要保证 API 查询返回的结果中，idle 标签的相关数据被排序在最后就可以。\n结果搜了下，发现官方的 promQL 根本没有这样的 sort by label 之类的函数。在 2014 年 Prometheus 自己团队的人就提了一个 Consider supporting sorting by label value 的 issue，觉得应该加入根据 label value 的排序功能，但是一直没什么下文。\n到了 2016 年，直接有人实现了它并提交了 PR，但是被拒了，理由是：\n概述一下就是要维护 promQL 的 纯洁性。\n嚯，看看这 👎 数量。\n然而隔壁的 VictoriaMetrics 的 MetricsQL 却实现了 4 个函数 1：\nsort_by_label sort_by_label_desc sort_by_label_numeric sort_by_label_numeric_desc 在 PR 的评论中，这条评论算是说出了大家的心声。生态中的两方互相推诿这个实现，社区用户迫切需要的功能总是卡在 promQL 这里。而且现在 Prometheus 的生态确实让人有点无语，功能特性上，官方只有两个不做: 这也不做，那也不做。搞的现在 Cortex、 Thanos、VictoriaMetrics 等无一不是选择将 Prometheus 这个单实例程序肢解成一个个模块来超越官方的功能局限。\n最后，如果官方的 promQL 支持 sort_by_label_desc 的话，CPU 面板的配置就简洁多了，配置一条查询语句即可：\nsort_by_label_desc( sum by(instance，mode) ( irate(node_cpu_seconds_total{instance=\u0026#34;$node\u0026#34;，job=\u0026#34;$job\u0026#34;}[1m]) ) / on (instance) group_left sum by (instance) ( irate(node_cpu_seconds_total{instance=\u0026#34;$node\u0026#34;，job=\u0026#34;$job\u0026#34;}[1m]) ), \u0026#34;mode\u0026#34; ) 基于上面这个美好的假设，使用 VictoriaMetrics 作为数据源测试，可以看见效果一切正常。\nMetricsQL\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-11-08","externalUrl":null,"permalink":"/posts/2022/11/promql-sort-by-label-feat/","section":"博客","summary":"最近给 Grafana 的 Node Exporter Full 这个公开仪表盘提了一个 PR.","title":"promQL 按 Label 排序的函数","type":"posts"},{"content":"","date":"2022-10-29","externalUrl":null,"permalink":"/tags/etcd/","section":"Tags","summary":"","title":"Etcd","type":"tags"},{"content":"Etcd 作为云原生领域知名的开源分布式 KV 存储系统，在存储的数据集较大的情况下一直存在一定的性能问题。所以在其官网文档页面上的 System limits 章节明确指出了建议给存储配置最大的限额是 8 GB：\n根据网上众多的资料指出，制约性能的主要关键是其存储引擎: bbolt。\nEtcd 目前使用的 bbolt 是 fork 自 Boltdb，原因是 Blotdb 的原作者把仓库 archive 了，README 中可以看见描述的理由是认为项目关键目标已经完成，后续没有精力继续维护。\n阿里巴巴在 2019 年对 bbolt 提交了一个 PR，还对应在 CNCF 的博客发文 Performance optimization of etcd in web scale data scenario 描述了这个改动的背景，过程和技术细节，号称改善了1000x的速度，让原先 Etcd 文档内提到的最大 8 GB 的文件系统限制不再存在，甚至可以让 Etcd 存储增长到 100 GB 也不会有性能衰减。\n从这个 PR 被 merge 的 commit 可以看见，受益的正式版本是 v1.3.2+。\n确定了这个 tag，追踪下 Etcd 的 go.mod 的提交记录就可以很轻松的找到具体是从什么版本开始引入这个变更的。从 vendor: update boltdb and grpc middleware version 可以看出，正式版本是在 v3.4.0+。\n不过很有意思的是，从开头指出的 v3.5 对应文档章节页面能够看出，其实文档并未更新关于 8 GB 限制的这个说法。在 Issues 里有人提 8GB recommended limit can be lifted 认为文档该更新了，但是就是还没更新。serathius（从此人简介看算是 Etcd 的技术决策者）在评论中指出需要个更官方的测试结果来支撑文档吹上的牛，所以这个估计还得要段时间。\n最后写到这，就挺纳闷的：对于 Etcd 这样一个项目来说，如此巨大的性能提升 patch，移除了这样一个限制，居然从 2019 到 2022，经过了数个 minor version，没有更新文档不说，甚至 v3.4.0 的 changelog 里都没提到，是个什么情况呢?\n","date":"2022-10-29","externalUrl":null,"permalink":"/posts/2022/10/etcd-big-data-performance/","section":"博客","summary":"Etcd 作为云原生领域知名的开源分布式 KV 存储系统，在存储的数据集较大的情况下一直存在一定的性能问题。所以在其官网文档页面上的 System limits 章节明确指出了建议给存储配置最大的限额是 8 GB：","title":"Etcd 的 Storage size limit","type":"posts"},{"content":"","date":"2022-10-23","externalUrl":null,"permalink":"/tags/fqdn/","section":"Tags","summary":"","title":"FQDN","type":"tags"},{"content":"","date":"2022-10-23","externalUrl":null,"permalink":"/tags/homelab/","section":"Tags","summary":"","title":"HomeLab","type":"tags"},{"content":"在 如何为你的私有网络选择 TLD(顶级域名) 中，描述了 TLD 的选择。这篇文章算是续集，讲讲服务器的命名。\nmnx.io 主张的最佳实践 # mnx.io 是一家主机托管商，其官网的公开博客上有一篇发表于 2014-06-10 的关于服务器命名的最佳实践博文：A Proper Server Naming Scheme。按文章的说法，这套最佳实践适合大多数的中小企业，可以轻松管理 1500+ 的服务器。我将通过下面的几个子章节简要概述一下文章的主张。\n使用 FQDN # 使用 DNS 系统，给主机设置真正的 FQDN。\n这意味着你需要有一个配置了 TLD 的本地 DNS 服务器。\n选择合适的 Label 设置 A 记录 # 选择好合适的 Label 后直接设置 A 记录。例如使用 crimson：\ncrimson.example.com. A 192.0.2.11 不理解 Label 这个术语的可以自行阅读 RFC 1035 2.3.1. Preferred name syntax，或者简单参考 Route 53 DNS domain name format 的文档摘抄：\nDomain names (including the names of domains, hosted zones, and records) consist of a series of labels separated by dots.\n通常情况下，Label 不应该表示出含有真正的目的或功能的词，而只是一个代表了唯一 ID 的名字、一个代号、一个标识符。\n应该如何选择具体的 Label，文章给出了一个词典：Oren Tirosh’s mnemonic encoding project，建议从其中随机挑选。这份词典中精心挑选的 1633 个单词具备以下特征：\n极短 (4 - 7 letters)，简洁明了 发音上容易区别，可以轻松通过语音交流识别（例如电话讨论），国际化交流无障碍 书面不易出现混淆问题，如拼写错误，字符互换等 字符互换的典型例子就是数字 0 和字母 O\n设置 CNAME 来附加信息 # 有了最基础的 FQDN，你就可以给它设置一个或多个别名 (CNAME) 来丰富这个服务器的功能详细信息，典型的有：\n地理位置 环境 (dev、test、prod) 组织架构 业务能力、代号 总之，在此处将附加信息保持规范、结构化就易于将其与 CMDB 关联，可以有效降低使用人员的脑力负担。并且得益于 DNS 的分层设计，在 DNS 记录上这些信息可以非常清晰的以层层递进的方式展现，例如：\n先使用 5 个字符的 联合国贸易和运输地点代码 UN / LOCODE 表示地理位置：\n\u0026lt;wip\u0026gt;.nyc.example.com. CNAME crimson.example.com. 接下来标识上环境：\n\u0026lt;wip\u0026gt;.prd.nyc.example.com. CNAME crimson.example.com. 最后，附上具体功能及序号：\nweb01.prd.nyc.example.com. CNAME crimson.example.com. 针对特殊情况的策略变化 # 当然, 总是存在一些特殊情况, 需要违反上述的准则进行灵活的调整。常见的情况有以下几种:\n专用网络和电力设备。由于此类设备硬件跟用途牢牢绑定, 可以在 Label 中使用功能单词的缩写直接指向 A 记录：\nvpn01.nyc.example.com. A 192.0.2.1 辅助 IP 和虚拟 IP。这类对象从本质上来说也是与功能/用途绑定的, 也建议与上述一样处理：\nsql01.nyc.example.com. A 192.0.2.1 邮件或 DNS 服务器。在 RFC 2181 的 10.3. MX and NS records 中明确提到, MX/NS 记录中配置的值不能是 CNAME 类型的 DNS 记录。这也就意味着邮件或 DNS 用途的服务器也只能使用 A 记录：\nmta01.example.com. A 192.0.2.20 配置 Domain Name Search # 最后，可以在每个机器上配置 搜索域，来方便在服务器间使用短名进行通信。例如配置 NY 地区的 prd 环境使用：\nsearch prd.nyc.example.com example.com 之后在其他服务器上发起的与这台服务器的网络通信（比如 ping、ssh 等命令）中就可以直接使用 sql01，而不是使用完整的 sql01.prd.nyc.example.com。\n结语 # 虽然方案看起来不会有多复杂，只是稍微需要有结构性的思考和使用/配置本地 DNS 服务的前提条件，有一点门槛。但正如 mnx.io 的文章所指出的那样，它在可用性、可维护性和对长期增长的支持之间取得了很好的平衡。建议在实践过程中可以尽量参考使用他们的 Tips \u0026amp; Tricks 作为指导。\n从理论上说，如果你的服务器数量超过了词典中的总词数 (1633)，那么在 Label 的选择上词典就没什么帮助了。我个人觉得，服务器数量超过 800 的话，或许可以考虑逐步转向使用特殊 ID 以适应未来的变化，也可以抛弃这个方案探索一条全新的道路。\n在 OpenWrt 上的实践 # 理论有了，接下来简单聊聊我在家庭网络中通过 OpenWrt 主路由上的 Dnsmasq 实现的具体实践。全部操作均通过 LuCI Web 页面进行。\nOpenWrt 版本是 v22.03.0。\n进入 网络 -\u0026gt; DHCP/DNS，在默认的 常规设置 标签页，可以看见配置的 TLD 是 .lan。\n进入 主机名映射 标签页，配置基础名字。\n我使用的主机名单词都是从上述提到的那个词典中挑选的，对应节点的详情是:\nHostname Motherboard OS Description unicorn Supermicro X10SLM-F PVE OpenWrt 的 VM 所在的宿主机 fiber ASUS P9A-I Debian 一台冷备用途的存储机器 axis Supermicro X10DRL-i PVE 家中主要的 Homelab 设备 接下来，转到 CNAME 标签页进行配置日常使用的真正名字.\n由于是家庭网络，就无需在 CNAME 的记录中使用地理位置、环境等信息，而是尽量选择个人易于记忆的，能显著代表功能的别名。例如 unicorn 实际使用的别名 netbox，就是因为它的主要用途是家里的主力网络处理设备，而 axis 的则是 homelab。\n除了本地的 .lan TLD 之外，图上也可看出，我还使用了真实的，目前由我持有的 .z10n0110.men 这个 TLD。因为有 Let\u0026rsquo;s Encrypt 签发来的免费证书，我选择将这个域名用在各个服务的 HTTPS 端点上，比如 PVE 的 WEB 控制台：\n配置完成后，使用 ping 测试一下：\n$ ping -c 2 axis PING axis.lan (192.168.15.6): 56 data bytes 64 bytes from 192.168.15.6: icmp_seq=0 ttl=64 time=10.607 ms 64 bytes from 192.168.15.6: icmp_seq=1 ttl=64 time=12.834 ms --- axis.lan ping statistics --- 2 packets transmitted，2 packets received，0.0% packet loss round-trip min/avg/max/stddev = 10.607/11.720/12.834/1.113 ms $ ping -c 2 homelab PING axis.lan (192.168.15.6): 56 data bytes 64 bytes from 192.168.15.6: icmp_seq=0 ttl=64 time=5.552 ms 64 bytes from 192.168.15.6: icmp_seq=1 ttl=64 time=5.422 ms --- axis.lan ping statistics --- 2 packets transmitted，2 packets received，0.0% packet loss round-trip min/avg/max/stddev = 5.422/5.487/5.552/0.065 ms 一切就绪后，只需要使用便宜的热敏打印机给每个机器贴上标签，标明最基础的/最关心的信息方便管理即可。\n","date":"2022-10-23","externalUrl":null,"permalink":"/posts/2022/10/naming-hosts-on-your-network/","section":"博客","summary":"在 如何为你的私有网络选择 TLD(顶级域名) 中，描述了 TLD 的选择。这篇文章算是续集，讲讲服务器的命名。","title":"如何给你的服务器命名","type":"posts"},{"content":"家中有 N 个 HomeLab 设备，上面常态化运行着一些 VM 提供服务/实验用途。数量一多，怎么访问服务地址变成了一个问题，因为 IP 地址实在是记不住。是时候给家中的网络统一配置一个私有的 TLD (顶级域名)了。\n选择什么 TLD 呢？RFC 是否有对应规范？很遗憾，搜索后发现，截至目前还没有 RFC 明确规范了预留给私有网络使用的 TLDs。那既然是私有网络，是不是随便用一个我想用的就行了呢？继续在网上搜索后发现这个 Top level domain/domain suffix for private network? 中的回答很有参考价值。\n首先是 Garret Wilson 的：\n不建议使用真实存在的域名的子域名\nRFC 6762 附录 G 中指出不少人在使用 .internal\nSAC113 调查报告指出应该为私有网络用途制定一个 TLD，同时不推荐使用 .internal\n这个奇怪的 zghjccbob3n0 域名是什么鬼？\ngavenkoa 则提到了超大规模公司的内部域名 TLD 选择：.internal。我搜索了一下 AWS 和 GCE 上一些产品文档，发现确实如此。\nAWS 在它提供的 EMR(Hadoop) 产品的文档页 Set up a VPC to host clusters 明确提到了可以使用 ec2.internal 或者其他 .internal 的域名来做节点的 FQDN 解析。\n而 GCE 更是在文档 Types of internal DNS names 中强烈建议在 FQDN 中加上 zone 的信息。\n虽然 .local 也是一个保留的 TLD，但是这个域名经常被用来做 mDNS 的多播，不建议使用。\n综合以上这些信息来看，其实从 ICANN 保留或者不会接受申请的 TLD 中选一个就行。推荐使用 RFC 6762 附录 G 中提到的：\nintranet internal private corp home lan 由于 OpenWrt 默认的本地 TLD 是 .lan，最后我也是偷懒就选择使用了 .lan。\n","date":"2022-10-22","externalUrl":null,"permalink":"/posts/2022/10/choose-tld-for-your-private-network/","section":"博客","summary":"家中有 N 个 HomeLab 设备，上面常态化运行着一些 VM 提供服务/实验用途。数量一多，怎么访问服务地址变成了一个问题，因为 IP 地址实在是记不住。是时候给家中的网络统一配置一个私有的 TLD (顶级域名)了。","title":"如何为你的私有网络选择 TLD（顶级域名）","type":"posts"},{"content":"","date":"2022-10-18","externalUrl":null,"permalink":"/tags/vnstat/","section":"Tags","summary":"","title":"Vnstat","type":"tags"},{"content":"有朋友问我是否能像 OpenWrt 上的 vnstat 那样，通过网页看 VPS 节点上 vnstat 生成的统计图。我个人的方案是通过比较简陋的脚本实现的（能用就行），已经在 VPS 上运行了很久了，这里简单分享一下具体过程。\n其实就是使用脚本调用 vnstati 生成对应的图片文件，再渲染出一个简单的 index.html 文件，最后通过 nginx 提供 Web 服务。\n简单的脚本 /opt/gen_vnstat_imgs.sh：\n#!/bin/sh VNSTAT_WWW=/data/vnstat_images VNSTAT_DB=/var/lib/vnstat BIN=/usr/bin/vnstati IMGS=\u0026#34;s h d t m\u0026#34; mkdir -p $VNSTAT_WWW interfaces=\u0026#34;$(ls -1 $VNSTAT_DB)\u0026#34; if [ -z \u0026#34;$interfaces\u0026#34; ]; then echo \u0026#34;No database found. Create new by command: vnstat -u -i eth0\u0026#34; exit 0 fi for interface in $interfaces; do if [ \u0026#34;$interface\u0026#34; = \u0026#34;vnstat.db\u0026#34; ]; then continue fi for imgs in $IMGS; do $BIN -${imgs} -i $interface -o $VNSTAT_WWW/vnstat_${interface}_${imgs}.png done done if [ ! -f $VNSTAT_WWW/index.html ]; then cat \u0026gt; $VNSTAT_WWW/index.html \u0026lt;\u0026lt; EOL \u0026lt;META HTTP-EQUIV=\u0026#34;refresh\u0026#34; CONTENT=\u0026#34;300\u0026#34;\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Traffic of Interface(s)\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; EOL for interface in $interfaces; do cat \u0026gt;\u0026gt; $VNSTAT_WWW/index.html \u0026lt;\u0026lt; EOL \u0026lt;h2\u0026gt;Traffic of Interface ${interface}\u0026lt;/h2\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026#34;vnstat_${interface}_s.png\u0026#34; alt=\u0026#34;${interface} Summary\u0026#34; /\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;img src=\u0026#34;vnstat_${interface}_h.png\u0026#34; alt=\u0026#34;${interface} Hourly\u0026#34; /\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td valign=\u0026#34;top\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;vnstat_${interface}_d.png\u0026#34; alt=\u0026#34;${interface} Daily\u0026#34; /\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td valign=\u0026#34;top\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;vnstat_${interface}_t.png\u0026#34; alt=\u0026#34;${interface} Top 10\u0026#34; /\u0026gt; \u0026lt;img src=\u0026#34;vnstat_${interface}_m.png\u0026#34; alt=\u0026#34;${interface} Monthly\u0026#34; style=\u0026#34;margin-top:5px;\u0026#34;/\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; EOL done cat \u0026gt;\u0026gt; $VNSTAT_WWW/index.html \u0026lt;\u0026lt; EOL \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; EOL fi 个人 VPS 上目前运行的是 Debian 11，部署在不同发行版的话，脚本中一些变量读者可以自行修改以匹配自己的实际情况。\n配置一下定时任务，定时生成新的统计图，我这里是 30 分钟一次：\n*/30 * * * * /opt/gen_vnstat_imgs.sh nginx 的配置非常简单：\nserver { listen *:443 ssl http2; server_name abc.com; location ^~ /bw { alias /data/vnstat_images/; index index.html; } } 最后附一张效果图，忽略这丑陋的排版，能用就行 :)\n","date":"2022-10-18","externalUrl":null,"permalink":"/posts/2022/10/show-vnstat-imgs-via-web/","section":"博客","summary":"有朋友问我是否能像 OpenWrt 上的 vnstat 那样，通过网页看 VPS 节点上 vnstat 生成的统计图。我个人的方案是通过比较简陋的脚本实现的（能用就行），已经在 VPS 上运行了很久了，这里简单分享一下具体过程。","title":"通过 Web 访问 vnstat 的统计图","type":"posts"},{"content":"","date":"2022-05-21","externalUrl":null,"permalink":"/tags/supermicro/","section":"Tags","summary":"","title":"Supermicro","type":"tags"},{"content":"在某海鲜二手市场买了一块超微的 X10DRL-i，用来做家里的 HomeLab。\n在超微的 X99 系列中选择它主要是因为紧凑的布局 (ATX) 和较多的 PICe 插槽。\n这篇文章将略去具体的 HomeLab 机器构建过程，主要记录一下配置这块主板的一些操作。\n升级 BIOS / BMC 固件 # 我手上这块主板的固件相当老旧，BIOS 版本是 2.0b，BMC 版本是 3.58。猜想是这块从机房淘汰出来的主板，在出厂到退役的这段时间内从未进行过固件更新。\n在对超微官网应产品固件页下载 BIOS / BMC 固件文件进行固件更新之前，需要先查看对应的 Release Notes，以确定清楚需要进行的操作步骤。\n最新的 BIOS 3.4 中没有看见特殊的注意事项，可以直接更新。而 BMC 3.91 的说明内描述了重要事项：\n即，为了不丢失 FRU 和配置信息，低于 3.73 的固件需要先升级到 3.80 再升级到 3.85+。\n3.80 固件从哪里获取呢？ 按说明中的描述，需要联系超微的官方支持来获取，这也符合超微官网并不提供历史固件下载的实际情况。不过这不要紧，因为 drunkencat.net 上专门提供了相当多的超微历史固件下载。\n图上可见，输入 X10DRL-i 进行查询，可以找到 3.80 的 BMC 固件和发行说明。查看发行说明并未发现如 3.91 那样的特殊要求，于是确定了 BMC 的更新流程：直接更新到 3.80，然后再更新到 3.91。\n更新固件的过程较简单，只需要下载固件压缩包，解压后阅读其中的说明文件了解清楚可用的平台和对应工具如何操作后，选择你喜好的工具执行即可。我个人选择的是在 FreeDOS 环境下使用 DOS 工具来更新 BIOS / BMC 固件。当然还有更便捷的方法，就是使用 BMC 上的固件更新功能，但这部分功能需要先使用许可证激活后才能使用。\n使用 OOB License 激活 BMC 功能 # 在 BIOS 中配置好 BMC 的 IP 地址后，可以登录 BMC 的网页进行基础体验。\n记得使用 HTTPS 协议，这块主板默认的 BMC 登录用户名密码是 ADMIN / ADMIN。\n为什么说是基础体验？ 因为 BMC 中那些强大的功能需要许对应的许可证才能激活，比如：\n通过 IPMI 或者网页进行 BIOS / BMC 固件的更新 模拟挂载 ISO 镜像 RAID 的配置及监控 强大的 RESTful API 支持 具体不同许可证之间的功能差异参考 System Management Software：\n上述提及的固件更新功能，就属于 OOB 许可证的能力范围，对应的售价是 $27。功能更多的 DCMS 的则要贵得多，售价 $180。\n超微这种连最基础的通过网页更新固件的能力都需要付费购买许可证来激活多少是有点吃相难看。好在世界上存在与我持相同意见的能人，因为 OOB 许可证的具体算法可以在互联网上轻松搜索到：\nSupermicro IPMI License Key (for updating BIOS) = HMAC-SHA1-96(INPUT: MAC address of BMC，SECRET KEY: 85 44 E3 B4 7E CA 58 F9 58 30 43 F8)\n就是使用 BMC 网口的 MAC 地址计算 HMAC。在 Linux / Unix 环境下可以通过 openssl 来生成许可证：\necho -n \u0026#39;{YOUR_BMC_MAC_ADDR}\u0026#39; | xxd -r -p | openssl dgst -sha1 -mac HMAC -macopt hexkey:8544E3B47ECA58F9583043F8 | awk \u0026#39;{print $2}\u0026#39; | cut -c 1-24 没有对应环境的可以使用 getmyoob 网页来生成。\n在 Miscellaneous -\u0026gt; Activate License 的页面中将许可证填入即可完成激活。\n需要重置许可证的话可以通过修改 BMC 的 MAC 地址来完成。信息来自官方的 IPMI_Users_Guide.pdf：\nNote 2: If BMC MAC address is changed，prior license keys will be lost.\n但是奇怪的是，我这块在 BIOS 中的 IPMI -\u0026gt; BMC Network Configuration 中 Station MAC Address 是灰色的，无法修改。\n我猜或许可以通过 ipmitool 来完成 (未测试)：\n# example to configure bmc lan 1 mac addr ipmitool lan set 1 macaddr 00:11:22:33:44:55 BMC Redfish，RESTful API # 对于 X10 系列主板，超微的 BMC 固件提供了 Redfish 支持，从下载的 BMC 固件文件名 REDFISH_X10_391_20200806_unsigned.zip 中就可以看见大写的 Redfish 这个单词。当然，超微对使用 Redfish 的最基本的要求是激活了 OOB 许可证。\n服务器主板上 BMC 最主要的能力就是以 IPMI 协议实现提供操作硬件的接口，但是自 IPMI 标准在 1994 年发布以来，逐渐暴露出以下问题：\n可扩展性不强 对用户不友好 协议及实现中的安全漏洞屡见不鲜 供应商能力无法对齐、碎片化、存在各种不兼容的 API、标准、协议和接口 由 DTMF 开发的 Redfish 作为取代 IPMI 的下一代协议，主要就是解决上述问题，提供更现代化的标准 API \u0026ndash; 使用 JSON 数据进行交互的 的 RESTful API。\n大概的架构可以参考这张网图：\n在这块主板上，激活 OOB 许可证后，就可以像这样调用账户管理的 API 获取账户信息：\n$ curl -sku \u0026#39;ADMIN:ADMIN\u0026#39; https://192.168.15.7/redfish/v1/AccountService/Accounts/2 | jq { \u0026#34;@odata.context\u0026#34;: \u0026#34;/redfish/v1/$metadata#ManagerAccount.ManagerAccount\u0026#34;, \u0026#34;@odata.type\u0026#34;: \u0026#34;#ManagerAccount.v1_0_0.ManagerAccount\u0026#34;, \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Accounts/2\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;User Account\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;User Account\u0026#34;, \u0026#34;Password\u0026#34;: \u0026#34;null\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;ADMIN\u0026#34;, \u0026#34;Locked\u0026#34;: false, \u0026#34;RoleId\u0026#34;: \u0026#34;Administrator\u0026#34;, \u0026#34;Enabled\u0026#34;: true, \u0026#34;Links\u0026#34;: { \u0026#34;Role\u0026#34;: { \u0026#34;@odata.id\u0026#34;: \u0026#34;/redfish/v1/AccountService/Roles/Administrator\u0026#34; } } } 具体的接口及操作指南可以查看 BMC 固件 zip 文件解压出的 Redfish_Ref_Guide_xxx.pdf。\n除了直接使用 cURL 此类 HTTP 客户端程序，你也可以选择 DMTF 官方出品的 Python Cli 程序 Redfishtool。\n很可惜的是 UpdateService 相关 API 需要 DCMS 的许可证才能工作。\n$ curl -k -u \u0026#39;ADMIN:ADMIN\u0026#39; https://192.168.15.7/redfish/v1/UpdateService/IPMIConfig | jq { \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;Base.v1_4_0.GeneralError\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;A general error has occurred. See ExtendedInfo for more information.\u0026#34;, \u0026#34;@Message.ExtendedInfo\u0026#34;: [ { \u0026#34;MessageId\u0026#34;: \u0026#34;Base.v1_4_0.OemLicenseNotPassed\u0026#34;, \u0026#34;Severity\u0026#34;: \u0026#34;Warning\u0026#34;, \u0026#34;Resolution\u0026#34;: \u0026#34;Please activate at least one license in above message.\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;Not licensed to perform this request. The following licenses DCMS were needed\u0026#34;, \u0026#34;MessageArgs\u0026#34;: [ \u0026#34;DCMS \u0026#34; ], \u0026#34;RelatedProperties\u0026#34;: [ \u0026#34;\u0026#34; ] } ] } } 这与查询到的超微 FAQ 回答一致。\nQ: What license is required to upload SSL using Redfish API？\nA: X10/X11 UP will be require DCMS and X11DP board will require OOB license.\n应该说相当多的 API 都需要使用 DCMS 许可证才能工作，倒也并不意外。\n如果许可证不是问题，BMC 中 HTTPS 证书的自动化更新就可以通过编写脚本调用 /redfish/v1/UpdateService/SSLCert 来完成，可以完美的做在 Let\u0026rsquo;s Encrypt 证书的自动化申请/更新的流程下游。\nBMC SMASH Shell # SMASH，一个同样是由 DTMF 开发的标准，这个东西从手册上看跟 Redfish 很像，都是结构化、树形的资源分类与管理，看起来区别只是提供了一个 SSH 协议的 adapter 方便使用脚本来实现简单的监控/管理功能。\n使用它只需要通过 SSH 协议登录即可，登录使用的账号密码就是 BMC 中配置的账号密码。登陆成功后进入的是一个简单的 Shell：\n可以使用 show {TARGET} 来查看具体对象的信息，如 show system1/sensors1/sensor001：\n具体的说明、能力和用法参考 IPMI_Users_Guide.pdf 中的 Appendix B: Using SMASH 部分。\nBMC 的 SSH 端口可以在 BMC 网页的 Configuration -\u0026gt; Port 中修改。\nPCIe 拆分 # 控制 PCIe 拆分的设置在 BIOS 中。与常用于家用的消费级主板不同，超微的这个配置入口稍微有些难找（缺乏经验），这块主板的手册页面也并没有详细说明。还是通过 FAQ，得知具体的配置路径是：\nAdvanced \u0026raquo; Chipset Configuration \u0026raquo; North Bridge \u0026raquo; IIO Configuration \u0026raquo; IIO1 / IIO2 configuration\n同时根据 FAQ 中的回答理解，可以拆分的 PCIe 插槽，上面就会有一个 IOUN（N 是数字）的选项。\n我只安装了一个 CPU，BIOS 中只有 IIO1 Configuration。\n这是我选择对 CPU1 SLOT2 进行了 x4x4 拆分后的实际 BIOS 界面截图(通过 BMC iKVM 功能远程操作)：\nBIOS 中所展示的 CPU1 SLOT1 ...、CPU1 SLOT2 ... 等与实际物理插槽的对应关系都在手册 X10DRL-i USER’S MANUAL 中。\n结合上述信息并实际操作 BIOS 后，确定这块主板支持拆分的插槽及能力是：\nPCIe Slot Form Factor Lanes Actually Connected Lanes Bifurcation Support CPU1 SLOT2 8 8 x4x4 CPU1 SLOT3 8 8 x4x4 CPU1 SLOT5 16 16 x4x4x4x4 / x8x8 / x8x4x4 如果你足够 丧 心 病 狂 的话，可以使用两块 X8-\u0026gt; X4X4 的 M.2 拆分适配卡，加上一块 X16 -\u0026gt; X4X4X4X4 的 M.2 拆分适配卡，光这三个插槽就能插上 8 块 NVMe SSD。\n还需要注意的是，实施任意插槽的拆分都会影响原 PCIe 总线地址号的分配。我这张 LSI 的 HBA 卡原先地址号是 02:00：\n另一个插槽拆分并安装上傲腾后，LSI HBA 卡的地址号变成了 03:00，原先的 02:00 被分配给了傲腾。\n上述截图来自 PVE 网页控制台。\n控制风扇转速 # 这块主板的 BIOS 界面没有地方可以调整风扇的设置，因为风扇全都由 BMC 进行控制。\n进入 BMC 网页的 Configuration -\u0026gt; Fan mode 页面中可以看见 4 个选项：\n我这块主板默认之前就配置的是上图所示的全速模式，而其他的 3 个模式找不到任何具体的文档定义及说明，只有一个 FAQ 回答了似是而非的答案：\nQ: For X10SRL-F, we found there are Heavy IO speed option in IPMI, what is this fan mode definition ? We couldn\u0026rsquo;t find any document states for it.\nA: X10SRL-F supports the following fan modes:\nStandard – Provide standard fan speed\nFull Speed – Maximum cooling\nOptimal – Balanced fan speed and power consumption\nHeavy I/O – Boost cooling to the add-on card zone\n我只在主板上连接了 2 个风扇：\nConnecter Name Type Usage Max RPM In Specification Max RPM Reported By BMC FAN2 Thermalright(利民) TL-D12B CPU 散热 1500±10% 1600 FAN6 Prolimatech(采融) PT12025(DB) 机箱后置出风 1500±10% 1400 实际对这 3 个模式测试观察到的结果是：\nFan Mode Behavior Standard 所有风扇低速运行几秒后突然满速运行一两秒，然后转速又逐渐降低，周而复始。 Optimal 同上 HeavyIO 同上 搜索并阅读 TrueNAS 论坛上的这篇 How To: Change IPMI Sensor Thresholds using ipmitool 帖子后才理解了这个现象。\n原因就是 BMC 中配置的传感器阈值 \u0026ndash; 所有风扇的 LNR (Lower Non-Recoverable) 阈值都在 15000 RPM（忘记截图当时的读数了）。非 Full 模式的风扇转速显然不是满速，而 BMC 定时监测到有风扇转速低于 Lower LNR 阈值后就会触发所有风扇进入全速模式。这个 15000 的阈值很明显是匹配其原始的工作负载对象 — 2U / 4U 机箱中的几万转大风量暴力风扇，我的风扇显然是水土不服。\n帖子中提到的几个阈值是：\nName Abbreviation Supermicro Term (Via BMC Web Console) Lower Non-Recoverable LNR Low NR Lower Critical LCR Low CT Lower Non-Critical LNC \\ Upper Non-Recoverable UNR High NR Upper Critical UCR High CT Upper Non-Critical UNC \\ 各传感器已设置的阈值可以在 BMC 网页的 Server Health -\u0026gt; Sensor Readings 中点击 Show Thresholds 按钮查看，或者使用 ipmitools：\nipmitool sensor list all 正如帖子中提出的那样，设置 BMC 中传感器各种阈值的手段最好是通过 ipmitool，因为超微的 BMC 网页上根本不存在设置的地方，而超微自己的 IPMI 工具又并不是总能正常工作。\n使用 ipmitool 调整两个风扇的 Lower LNR、LCR、LNC 以匹配我的实际情况后，风扇周而复始的转速波动情况不再出现。\nipmitool sensor thresh FAN2 lower 0 100 200 ipmitool sensor thresh FAN6 lower 0 100 200 而来自 servethehome 的这篇 Supermicro X9/X10/X11 Fan Speed Control 帖子则代替超微官方补充描述了 Standard、Optimal、HeavyIO 这 3 种模式的具体定义：\n于是实际测试了下这几个模式下风扇的 RPM，结果如下：\nFan Mode FAN2 RPM, Ratio FAN6 RPM, Ratio Standard 800, = 50% 700, = 50% Optimal 700, ≌ 43% 600, ≌ 42% HeavyIO 900, ≌ 56% 600, ≌ 42% 不光与帖子中的描述不是完全一致，更让人纳闷的是，在这几个模式中来回反复切换几次就会出现：同一个模式下第 1 次和第 N 次的结果不是完全一致的现象。猜测 BMC 不仅仅是按固定百分比去控制转速，可能还与对应 Zone 的温度有关。\n好在帖子中也给出了自定义控制风扇转速的方法，这也是我最后采用的方式。简要来说，需要做到：\nBMC 的 Fan Mode 配置为 Full 发送 IPMI RAW 指令指定具体风扇转速比例 除了通过 BMC 网页上设置风扇模式为 Full，也可以通过 ipmitool 工具进行：\n# Gets the fan mode # 0/1/2/4 Standard/Full/Optimal/HeavyIO ipmitool raw 0x30 0x45 0x00 # Sets the fan mode to Standard ipmitool raw 0x30 0x45 0x01 0x00 # Sets the fan mode to Full ipmitool raw 0x30 0x45 0x01 0x01 # Sets the fan mode to Optimal ipmitool raw 0x30 0x45 0x01 0x02 # Sets the fan mode to HeavyIO ipmitool raw 0x30 0x45 0x01 0x04 根据帖子描述，RAW 指令中需要使用 Hex 来描述转速百分比，0x64 是十进制的 100，就是表示 100%，0x32 表示 50%，依此类推。例如设置 CPU zone (0) 的风扇使用 50% 转速：\nipmitool raw 0x30 0x70 0x66 0x01 0x00 0x32 但是，实际观察到的情况又与帖子存在出入。¯\\_(ツ)_/¯\n0x64 确实是满速，而 0x32 在我执行指令后通过 ipmitool sdr 观察发现，CPU zone 的 FAN2、FAN6 风扇转速分别是 1000 和 900，接近 64% 的比例。\n$ ipmitool raw 0x30 0x70 0x66 0x01 0x00 0x32 $ ipmitool sdr | grep FAN FAN1 | no reading | ns FAN2 | 1000 RPM | ok FAN3 | no reading | ns FAN4 | no reading | ns FAN5 | no reading | ns FAN6 | 900 RPM | ok FANA | no reading | ns FANB | no reading | ns 这个 3.91 的 BMC 是 2021 年的，servethehome 帖子的最后修改时间是 2016，可能是超微又改了什么东西导致的。\n反复测试后发现只有 0x24 才符合 50% 转速: FAN2 800 RPM，FAN6 700 RPM，且多次结果完全一致。基于静音和散热的考量，最终选择的也是这个值。\n通过 RAW 指令设置转速的缺点是，设置在 BMC Reset (掉电/重置) 后会丢失。如果你像我一样使用的是 Linux (比如 PVE)，可以选择使用 Systemd 的 oneshot unit 来做到在系统启动时自动设置：\n$ systemctl cat ipmi_fan.service # /etc/systemd/system/ipmi_fan.service [Unit] Description=ipmi_fan script After=openipmi.service [Service] Type=oneshot User=root Group=root ExecStart={PATH_TO_YOUR_SCRIPT} KillSignal=SIGQUIT TimeoutStopSec=5 KillMode=process [Install] WantedBy=multi-user.target 想要手动触发 BMC Reset 可以使用 BMC 网页上的 Maintenance -\u0026gt; Unit Reset，或者使用 ipmitool：\nipmitool mc reset cold Github 上有个 Python 项目 smfc (Super Micro fan control for Linux (home) servers.)，号称可以做到按需自动调整风扇转速，但我没有用过。\n","date":"2022-05-21","externalUrl":null,"permalink":"/posts/2022/05/using-supermicro-x10drl-i/","section":"博客","summary":"在某海鲜二手市场买了一块超微的 X10DRL-i，用来做家里的 HomeLab。","title":"使用 Supermicro X10DRL-i","type":"posts"},{"content":"","date":"2021-05-30","externalUrl":null,"permalink":"/tags/oom/","section":"Tags","summary":"","title":"OOM","type":"tags"},{"content":"在使用 Linux 的实际生产实践中，其上运行的程序有可能会发生被 OOM Kill 的情况。某些场景下，我们会想把它监控起来。\nPrometheus 生态中的 node_exporter 在 v0.16.0 开始加强了对 vmstat 的 parse \u0026amp; collect，默认情况下新增了一个关键的 Metric：node_vmstat_oom_kill。\n它本质上是一个 Counter。并且这个 Metric 的是否存在其实依赖于 Linux Kernel 的版本: 只有版本在 4.13+ 的内核才会通过 /proc/vmstat 暴露这个 oom_kill 计数信息。\n多数情况下这个 Metric 也够用。但，如果你不只是想要这样单纯的 OOM 计数，而是想采集具体的 OOM 信息，比如采集并结构化存储至 ELK 或者其他什么的系统，再扩展出告警，通知等功能，可以在 rsyslog 中使用 imkmsg 模块，精细化定义一套复杂的 input / parse / output 等逻辑实现。\n如果习惯自己编程而不想接触 rsyslog 那一堆 DSL、配置语法等，也可以从 imkmsg 模块的信息源头处 /dev/kmsg 自行采集、解析、处理来进行。/dev/kmsg 输出的信息是高度结构化的，格式为 \u0026quot;level,sequnum,timestamp;\u0026lt;message text\u0026gt;\\n\u0026quot;，更多信息可以参考 Rsyslog 的 imkmsg 文档。\n为什么不是选择对 /proc/kmsg 进行解析，是因为 /proc/kmsg 是一次性缓冲的，读取过的数据无法再次读取，而 /dev/kmsg 支持多次重复读取。\n当然，如果是习惯使用 golang 的读者，也可以直接使用这个开源的 parser：go-kmsg-parser。\n","date":"2021-05-30","externalUrl":null,"permalink":"/posts/2021/05/monitoring-oom-kill/","section":"博客","summary":"在使用 Linux 的实际生产实践中，其上运行的程序有可能会发生被 OOM Kill 的情况。某些场景下，我们会想把它监控起来。","title":"监控 Linux 上的 OOM","type":"posts"},{"content":"","date":"2019-01-06","externalUrl":null,"permalink":"/tags/collectd-web/","section":"Tags","summary":"","title":"Collectd-Web","type":"tags"},{"content":"我有一个 VPS，内存只有 237 MiB，上面运行的是 Debian 9。供应商自己的监控功能和页面只能评价为存在. 我想自己监控下机器的一些指标，用 Prometheu 生态的 node_exporter + grafana 显然不合适，因为资源实在是捉襟见肘。\n这时候就需要相对来说比较古老的那些使用 RRD 技术的监控采集软件上场了。我选择的是使用 collectd，具体一套方案是：\ncollectd 负责采集数据 collectd-web 来提供 Web 的 UI/UX fcgiwrap 处理 fcgi nginx 做 Web Server 下面简要记录一下对应程序的安装过程和配置。\ncollectd 直接使用 apt 安装。安装完后，可以对照着文档去编辑 /etc/collectd/collectd.conf，配置开启想要的插件。我配置了以下插件:\nLoadPlugin syslog \u0026lt;Plugin syslog\u0026gt; LogLevel info \u0026lt;/Plugin\u0026gt; LoadPlugin conntrack LoadPlugin contextswitch LoadPlugin cpu LoadPlugin cpufreq LoadPlugin df LoadPlugin disk LoadPlugin entropy LoadPlugin interface LoadPlugin iptables LoadPlugin irq LoadPlugin load LoadPlugin memory LoadPlugin netlink LoadPlugin processes LoadPlugin rrdtool LoadPlugin swap LoadPlugin tcpconns LoadPlugin uptime LoadPlugin users \u0026lt;Plugin df\u0026gt; FSType rootfs FSType sysfs FSType proc FSType devtmpfs FSType devpts FSType tmpfs FSType fusectl FSType cgroup IgnoreSelected true \u0026lt;/Plugin\u0026gt; \u0026lt;Plugin memory\u0026gt; ValuesAbsolute true ValuesPercentage true \u0026lt;/Plugin\u0026gt; \u0026lt;Plugin rrdtool\u0026gt; DataDir \u0026#34;/var/lib/collectd/rrd\u0026#34; \u0026lt;/Plugin\u0026gt; \u0026lt;Plugin tcpconns\u0026gt; ListeningPorts true AllPortsSummary false \u0026lt;/Plugin\u0026gt; 接着 collectd-web 的安装，我选择直接使用 git 仓库的部署模式，从 github clone 下来：\n[root@vps collectd-web]# git remote -v origin https://github.com/httpdss/collectd-web.git (fetch) origin https://github.com/httpdss/collectd-web.git (push) fcgi 的工作交给 fcgiwrap，同样直接使用 apt 安装即可，记得启用对应的 socket unit。\n[root@vps nginx]# dpkg -S /lib/systemd/system/fcgiwrap.socket fcgiwrap: /lib/systemd/system/fcgiwrap.socket [root@vps nginx]# systemctl enable fcgiwrap.socket 最后配置 nginx，我这里 collectd-web 是 git clone 在了 /opt/collectd-web，读者需要自行修改以匹配自己的实际情况。\nserver { listen *:443 ssl http2; ssl_certificate x.crt; ssl_certificate_key x.key; server_name abc.com; charset utf-8; auth_basic \u0026#34;Restricted area\u0026#34;; auth_basic_user_file auth.db; location / { default_type text/html; root /opt/collectd-web; } location /cgi-bin/ { gzip off; root /opt/collectd-web; fastcgi_pass unix:/run/fcgiwrap.socket; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; } } ","date":"2019-01-06","externalUrl":null,"permalink":"/posts/2019/01/monitoring-linux-node-with-collected-web/","section":"博客","summary":"我有一个 VPS，内存只有 237 MiB，上面运行的是 Debian 9。供应商自己的监控功能和页面只能评价为存在.","title":"使用 Collectd-web 监控 Linux 节点","type":"posts"},{"content":"","date":"2018-01-09","externalUrl":null,"permalink":"/tags/bcache/","section":"Tags","summary":"","title":"Bcache","type":"tags"},{"content":"bcache 是 Linux 内核级别提供的一种缓存方案(要求内核版本 3.10+ 并且内核编译时内部有加入 bcache 支持), 可以使用一个 SSD 的分区或整块硬盘空间作为另一个大容量 HDD 分区的特定缓存来加速 HDD 的随机读写操作.\n下面将记录在 CentOS 7 上安装并配置的实验过程.\ninstall # CentOS 7 官方并未提供 bcache 的模块支持, 可以自己编译 bcache 模块或者使用别人已打包好的 rpm 源.\n自行编译 bcache 模块需要整个 linux kernel source, 使用官方发行版内核来编译模块的话, 根据官方指导获得你需要的版本的 源码\n这里初略概括下根据官方指导的如何 编译模块, 我们这里选择第一种 build *.ko 的方式\n在 kernel 源码目录, 我们首先需要获取你期望编译的 kernel 版本的 kernel configuration 文件:\nModule.symvers .config # 命令来自 https://wiki.archlinux.org/index.php/Compile_kernel_module $ cp /usr/lib/modules/$(uname -r)/build/.config ./ $ cp /usr/lib/modules/$(uname -r)/build/Module.symvers ./ 如果没复制 Module.symvers, 编译会出现 no symbol version for module_layout 错误.\n上面命令的使用了 uname -r 来直接定位到当前系统运行的内核版本的 source 目录, 如果你不是编译当前运行的内核的 bcache 模块可自行指定 kernel 的 source 目录.\nmake oldconfig 后再 make menuconfig, 根据文档找到 bcache 的配置,将其选择为 M, 表示编译为模块.\n其他步骤按官方步骤执行, 若中途出错一般是未安装某些软件, 可以根据出错提示, 安装对应软件解决.\n模块编译后, 还要根据发行版的约束将模块设置为 boot 时加载, 需要在 /etc/modules-load.d/ 目录下新建 /etc/modules-load.d/bcache.conf, 加入内容\n# Load bcache.ko at boot # See the modules-load.d(5) and systemd-modules-load.service(8) man pages for more information. bcache bcache-tools # 内核模块编译完成后, 还需要 userspace tools 的支持, 就是 bcache-tools 软件包.\n此软件包不光提供了常用的命令行工具, 还会自动的生成 udev rules.\n目前 red hat 系和 debian 系的 bcache-tools 工具集里的 udev rules 等略有差别, 源码及下载地址都不一样, fedora 20 以上的系统源里已包含此工具集, 包名就叫 bcache-tools\n具体不同发行版的安装方式, 从 bcache 的 官方网址 摘抄可见\nBcache has been merged into the mainline Linux kernel; for the latest stable bcache release use the latest 3.10 or 3.11 stable kernel.\nFor the userspace tools,\ngit clone http://evilpiepirate.org/git/bcache-tools.git\nThe udev rules, Debian/Ubuntu source package, and Ubuntu PPA are maintained here:\ngit clone https://github.com/g2p/bcache-tools.git\nTo use the PPA (Ubuntu Raring and Saucy):\nsudo add-apt-repository ppa:g2p/storage sudo apt-get update sudo apt-get install bcache-tools\nThe PPA also contains blocks, a conversion tool.\nA Fedora package is available in Fedora 20, and maintained here.\nmake-bcache # 安装完成用户态工具 bcache-tools 后, 使用其中的 make-bcache 来操作磁盘\n[root@pure-test ~]# make-bcache --help Usage: make-bcache [options] device -C, --cache Format a cache device -B, --bdev Format a backing device -b, --bucket bucket size -w, --block block size (hard sector size of SSD, often 2k) -o, --data-offset data offset in sectors --cset-uuid UUID for the cache set --writeback enable writeback --discard enable discards --cache_replacement_policy=(lru|fifo) -h, --help display this help and exit 对设备进行操作前需要先清理整个设备的分区及其他信息:\n# wipefs -a \u0026#39;devices\u0026#39; wipefs -a /dev/sda2 # 分区 wipefs -a /dev/sdb # 整个磁盘 device 可以是整个磁盘, 也可以是一个分区\n假设 /dev/sdb 是一块ssd, /dev/sda 是一块HDD, 现在使用 /dev/sdb1 作为 cache, /dev/sda2 作为 backend, 2 个分区都已经使用 wipefs 处理过. 接下来就开始分别创建 cache 设备和 backend 设备.\ncache # 生成 cache 设备\n[root@pure-test ~]# make-bcache -C /dev/sdb1 UUID: bf8f710d-5c99-4d40-8747-00d72a5af2cf Set UUID: 5da393d6-b157-4bb3-a105-235a1425894d version: 0 nbuckets: 244206 block_size: 1 bucket_size: 1024 nr_in_set: 1 nr_this_dev: 0 first_bucket: 1 UUID \u0026ndash;\u0026gt; 实体设备 /dev/sdb1 的 UUID\nSet UUID \u0026ndash;\u0026gt; 此 cache 设备被虚拟的 UUID \u0026ndash;\u0026gt; 5da393d6-b157-4bb3-a105-235a1425894d\n此 UUID 还可以通过查看 /sys/fs/bcache/ 来确定\n[root@pure-test ~]# ls -lh /sys/fs/bcache/ total 0 drwxr-xr-x 7 root root 0 Sep 28 15:15 5da393d6-b157-4bb3-a105-235a1425894d --w------- 1 root root 4.0K Sep 28 15:15 register --w------- 1 root root 4.0K Sep 28 15:15 register_quiet 查看此 cache 设备对应的实体设备\n[root@pure-test ~]# ls -lh /sys/fs/bcache/5da393d6-b157-4bb3-a105-235a1425894d/ total 0 -r--r--r-- 1 root root 4.0K Sep 29 10:49 average_key_size -r--r--r-- 1 root root 4.0K Sep 29 10:49 block_size -r--r--r-- 1 root root 4.0K Sep 29 10:49 btree_cache_size -r--r--r-- 1 root root 4.0K Sep 29 10:49 bucket_size lrwxrwxrwx 1 root root 0 Sep 29 10:49 cache0 -\u0026gt; ../../../devices/pci0000:00/0000:00:1f.2/ata6/host5/target5:0:0/5:0:0:0/block/sdb/sdb1/bcache -r--r--r-- 1 root root 4.0K Sep 29 10:49 cache_available_percent --w------- 1 root root 4.0K Sep 29 10:49 clear_stats -r--r--r-- 1 root root 4.0K Sep 29 10:49 congested -rw-r--r-- 1 root root 4.0K Sep 29 10:49 congested_read_threshold_us -rw-r--r-- 1 root root 4.0K Sep 29 10:49 congested_write_threshold_us -rw-r--r-- 1 root root 4.0K Sep 29 10:49 errors --w------- 1 root root 4.0K Sep 29 10:49 flash_vol_create drwxr-xr-x 2 root root 0 Sep 29 10:49 internal -rw-r--r-- 1 root root 4.0K Sep 29 10:49 io_error_halflife -rw-r--r-- 1 root root 4.0K Sep 29 10:49 io_error_limit -rw-r--r-- 1 root root 4.0K Sep 29 10:49 journal_delay_ms -r--r--r-- 1 root root 4.0K Sep 29 10:49 root_usage_percent drwxr-xr-x 2 root root 0 Sep 29 10:49 stats_day drwxr-xr-x 2 root root 0 Sep 29 10:49 stats_five_minute drwxr-xr-x 2 root root 0 Sep 29 10:49 stats_hour drwxr-xr-x 2 root root 0 Sep 29 10:49 stats_total --w------- 1 root root 4.0K Sep 29 10:49 stop -rw-r--r-- 1 root root 4.0K Sep 29 10:49 synchronous -r--r--r-- 1 root root 4.0K Sep 29 10:49 tree_depth --w------- 1 root root 4.0K Sep 29 10:49 unregister 可以看见此块 cache 设备对应的是 /dev/sdb1\nbcakend # 以下命令会在 /dev 下生成一个名为 bcache0 的 backend 设备\n[root@pure-test ~]# make-bcache -B /dev/sda2 UUID: eaad4f10-2e58-4379-ba32-75deaa57eda0 Set UUID: 3246bd30-130d-4a5b-b1ff-e71fe37bedf4 version: 1 block_size: 1 data_offset: 16 backend 设备一旦正常生成,就可对其进行普通的格式化等操作,与操作普通硬盘分区一般\nmkfs.ext4 /dev/bcache0 show real device # 查看此后端 bcache0 设备实际对应的实体磁盘\n当 /dev/bcache0 未格式化文件系统时\n[root@pure-test ~]# ls -lh /dev/bcache/by-uuid/ total 0 lrwxrwxrwx 1 root root 13 Sep 29 15:45 eaad4f10-2e58-4379-ba32-75deaa57eda0 -\u0026gt; ../../bcache0 可以看见此设备实际指向的是 uuid eaad4f10-2e58-4379-ba32-75deaa57eda0 的设备\n[root@pure-test ~]# ls -lh /dev/disk/by-uuid/ total 0 lrwxrwxrwx 1 root root 10 Sep 29 15:38 388e01f1-a27c-4f6c-bcac-823962963259 -\u0026gt; ../../sda1 lrwxrwxrwx 1 root root 10 Sep 29 15:38 71683e71-2990-48a1-840d-0f43fb11340c -\u0026gt; ../../sda5 lrwxrwxrwx 1 root root 10 Sep 29 15:38 abeedced-9450-4159-927b-5b4298d96bc1 -\u0026gt; ../../sda3 lrwxrwxrwx 1 root root 10 Sep 29 15:38 bf8f710d-5c99-4d40-8747-00d72a5af2cf -\u0026gt; ../../sdb1 lrwxrwxrwx 1 root root 10 Sep 29 15:45 eaad4f10-2e58-4379-ba32-75deaa57eda0 -\u0026gt; ../../sda2 lrwxrwxrwx 1 root root 10 Sep 29 15:38 f1982595-8044-48ae-a344-54836d487666 -\u0026gt; ../../sda6 lrwxrwxrwx 1 root root 13 Sep 29 16:30 ff8e456b-a5a0-4d20-8697-241e451e486e -\u0026gt; ../../bcache0 可发现此 UUID 对应为 /dev/sda2\n当 /dev/bcache0 已经格式化文件系统后\n直接查看 /sys/block/bcache0/\n[root@pure-test ~]# ls -lh /sys/block/bcache0/ total 0 -r--r--r-- 1 root root 4.0K Sep 29 10:53 alignment_offset lrwxrwxrwx 1 root root 0 Sep 29 10:53 bcache -\u0026gt; ../../../pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda/sda2/bcache lrwxrwxrwx 1 root root 0 Sep 29 10:53 bdi -\u0026gt; ../../bdi/253:0 -r--r--r-- 1 root root 4.0K Sep 29 10:53 capability -r--r--r-- 1 root root 4.0K Sep 29 10:53 dev -r--r--r-- 1 root root 4.0K Sep 29 10:53 discard_alignment -r--r--r-- 1 root root 4.0K Sep 29 10:53 ext_range drwxr-xr-x 2 root root 0 Sep 29 10:53 holders -r--r--r-- 1 root root 4.0K Sep 29 10:53 inflight drwxr-xr-x 2 root root 0 Sep 29 10:53 power drwxr-xr-x 2 root root 0 Sep 29 10:53 queue -r--r--r-- 1 root root 4.0K Sep 29 10:53 range -r--r--r-- 1 root root 4.0K Sep 29 10:53 removable -r--r--r-- 1 root root 4.0K Sep 29 10:53 ro -r--r--r-- 1 root root 4.0K Sep 29 10:53 size drwxr-xr-x 2 root root 0 Sep 29 10:53 slaves -r--r--r-- 1 root root 4.0K Sep 29 10:53 stat lrwxrwxrwx 1 root root 0 Sep 29 10:53 subsystem -\u0026gt; ../../../../class/block drwxr-xr-x 2 root root 0 Sep 29 10:53 trace -rw-r--r-- 1 root root 4.0K Sep 29 10:53 uevent 可以看见此后端设备实际对应的是 /dev/sda2\nbackend 设备 bcache0 与一般硬盘表现看起来无异, 可通过常规的 UUID 查看法查看其 UUID\n# 未格式化 /dev/bcache0 设备时, 设备是未拥有 uuid 的, 格式化后才拥有 uuid [root@pure-test ~]# blkid /dev/sda1: UUID=\u0026#34;388e01f1-a27c-4f6c-bcac-823962963259\u0026#34; TYPE=\u0026#34;ext4\u0026#34; /dev/sda3: UUID=\u0026#34;abeedced-9450-4159-927b-5b4298d96bc1\u0026#34; TYPE=\u0026#34;ext4\u0026#34; /dev/sda5: UUID=\u0026#34;71683e71-2990-48a1-840d-0f43fb11340c\u0026#34; TYPE=\u0026#34;ext4\u0026#34; /dev/sda6: UUID=\u0026#34;f1982595-8044-48ae-a344-54836d487666\u0026#34; SEC_TYPE=\u0026#34;ext2\u0026#34; TYPE=\u0026#34;ext3\u0026#34; /dev/block/8:3: UUID=\u0026#34;abeedced-9450-4159-927b-5b4298d96bc1\u0026#34; TYPE=\u0026#34;ext4\u0026#34; /dev/bcache0: UUID=\u0026#34;ff8e456b-a5a0-4d20-8697-241e451e486e\u0026#34; TYPE=\u0026#34;ext4\u0026#34; [root@pure-test ~]# ls -lh /dev/disk/by-uuid/ total 0 lrwxrwxrwx 1 root root 10 Sep 29 15:38 388e01f1-a27c-4f6c-bcac-823962963259 -\u0026gt; ../../sda1 lrwxrwxrwx 1 root root 10 Sep 29 15:38 71683e71-2990-48a1-840d-0f43fb11340c -\u0026gt; ../../sda5 lrwxrwxrwx 1 root root 10 Sep 29 15:38 abeedced-9450-4159-927b-5b4298d96bc1 -\u0026gt; ../../sda3 lrwxrwxrwx 1 root root 10 Sep 29 15:38 bf8f710d-5c99-4d40-8747-00d72a5af2cf -\u0026gt; ../../sdb1 lrwxrwxrwx 1 root root 10 Sep 29 15:45 eaad4f10-2e58-4379-ba32-75deaa57eda0 -\u0026gt; ../../sda2 lrwxrwxrwx 1 root root 10 Sep 29 15:38 f1982595-8044-48ae-a344-54836d487666 -\u0026gt; ../../sda6 lrwxrwxrwx 1 root root 13 Sep 29 16:30 ff8e456b-a5a0-4d20-8697-241e451e486e -\u0026gt; ../../bcache0 eaad4f10-2e58-4379-ba32-75deaa57eda0 就是后端 HDD 存储 bcache0 的 UUID\none step # 可通过将一行指令同时生产 cacahe 和 backend 设备\nmake-bcache -B /dev/sda2 -C /dev/sdb1 指定相关参数\nmake-bcache --wipe-bcache -w512 --discard --writeback -B /dev/sda2 -C /dev/sdb1 attach \u0026amp; detach # 在 bcache-tools 编译安装时, 会安装相应的 udev rules, 会将 cache 设备与 backend 设备自动进行 attch (绑定).\n相应的, 我们也可以进行手动 attch 或 detach\nattach # #attach echo 5da393d6-b157-4bb3-a105-235a1425894d \u0026gt; /sys/block/bcache0/bcache/attach 即将 cache 设备的 UUID 写入到 backend (bcache0) 设备的 /sys/block/{设备}/bcache/attach 中\n执行此命令后,会在 /sys/block/bcache0/bcache/ 下生成一个 cache 目录 实际指向为 cache 设备目录 就表示已经 attch 成功\n[root@pure-test ~]# ls -lh /sys/block/bcache0/bcache/ total 0 --w------- 1 root root 4.0K Sep 28 16:01 attach -rw-r--r-- 1 root root 4.0K Sep 28 15:13 bypass_torture_test lrwxrwxrwx 1 root root 0 Sep 28 16:01 cache -\u0026gt; ../../../../../../../../../../../fs/bcache/5da393d6-b157-4bb3-a105-235a1425894d -rw-r--r-- 1 root root 4.0K Sep 28 15:13 cache_mode --w------- 1 root root 4.0K Sep 28 15:13 clear_stats --w------- 1 root root 4.0K Sep 28 16:01 detach lrwxrwxrwx 1 root root 0 Sep 28 15:13 dev -\u0026gt; ../../../../../../../../../../virtual/block/bcache0 -r--r--r-- 1 root root 4.0K Sep 28 15:13 dirty_data -rw-r--r-- 1 root root 4.0K Sep 28 15:13 label -r--r--r-- 1 root root 4.0K Sep 28 15:13 partial_stripes_expensive -rw-r--r-- 1 root root 4.0K Sep 28 15:13 readahead -rw-r--r-- 1 root root 4.0K Sep 28 15:13 running -rw-r--r-- 1 root root 4.0K Sep 28 15:13 sequential_cutoff -r--r--r-- 1 root root 4.0K Sep 28 15:13 state drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_day drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_five_minute drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_hour drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_total --w------- 1 root root 4.0K Sep 28 15:13 stop -r--r--r-- 1 root root 4.0K Sep 28 15:13 stripe_size -rw-r--r-- 1 root root 4.0K Sep 28 15:13 verify -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_delay -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_metadata -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_percent -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate -r--r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_debug -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_d_term -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_p_term_inverse -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_update_seconds -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_running 可以看见 cache 目录, 已经 attch 成功\ndetach # #detach echo 5da393d6-b157-4bb3-a105-235a1425894d \u0026gt; /sys/block/bcache0/bcache/detach 相应对于 attach, detach 成功之后 /sys/block/bcache0/bcache/ 下的 cache 目录就会消失\n[root@pure-test ~]# ls -lh /sys/block/bcache0/bcache/ total 0 --w------- 1 root root 4.0K Sep 28 16:01 attach -rw-r--r-- 1 root root 4.0K Sep 28 15:13 bypass_torture_test -rw-r--r-- 1 root root 4.0K Sep 28 15:13 cache_mode --w------- 1 root root 4.0K Sep 28 15:13 clear_stats --w------- 1 root root 4.0K Sep 28 16:01 detach lrwxrwxrwx 1 root root 0 Sep 28 15:13 dev -\u0026gt; ../../../../../../../../../../virtual/block/bcache0 -r--r--r-- 1 root root 4.0K Sep 28 15:13 dirty_data -rw-r--r-- 1 root root 4.0K Sep 28 15:13 label -r--r--r-- 1 root root 4.0K Sep 28 15:13 partial_stripes_expensive -rw-r--r-- 1 root root 4.0K Sep 28 15:13 readahead -rw-r--r-- 1 root root 4.0K Sep 28 15:13 running -rw-r--r-- 1 root root 4.0K Sep 28 15:13 sequential_cutoff -r--r--r-- 1 root root 4.0K Sep 28 15:13 state drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_day drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_five_minute drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_hour drwxr-xr-x 2 root root 0 Sep 28 15:13 stats_total --w------- 1 root root 4.0K Sep 28 15:13 stop -r--r--r-- 1 root root 4.0K Sep 28 15:13 stripe_size -rw-r--r-- 1 root root 4.0K Sep 28 15:13 verify -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_delay -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_metadata -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_percent -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate -r--r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_debug -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_d_term -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_p_term_inverse -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_rate_update_seconds -rw-r--r-- 1 root root 4.0K Sep 28 15:13 writeback_running cache mode # 可以通过查看目录下的 cache_mode 文件来查看 cache 的缓存策略\n[root@pure-test ~]# cat /sys/block/bcache0/bcache/cache_mode writethrough [writeback] writearound none 修改策略\n[root@pure-test ~]# echo writethrough \u0026gt; /sys/block/bcache0/bcache/cache_mode [root@pure-test ~]# cat /sys/block/bcache0/bcache/cache_mode [writethrough] writeback writearound none 强制让 backend 设备在无 cache 设备情况下运行\n[root@pure-test ~]# echo 1 \u0026gt; /sys/block/sda/sda2/bcache/running 此处不能使用 /sys/block/bcache0, 因为此时 bcach0 并不存在.\n假如你的 backend 设备未使用分区, 则 bcache 目录为 /sys/block/sda/bcache\nstop bcache # 停止 bcache\n[root@pure-test ~]# echo 1 \u0026gt; /sys/block/sda/sda2/bcache/stop bcache-status # 可以通查看 bcache 目录下的 state 文件来查看 bcache 的状态\ncat /sys/block/bcache0/bcache/state 也可以通过 python 写的 bcache-status 小程序来查看 bcache 的状态 此工具包含在 fedora 20 以上的 bcache-tools 工具里, debian 系或自行编译的 bcache-tools 里并未有此工具, 需要自行上 github 下载.\n[root@pure-test ~]# bcache-status --help usage: bcache-status [--help] [-f] [-h] [-d] [-t] [-a] [-r] [-s] [-g] optional arguments: --help Show this help message and exit -f, --five-minute Print the last five minutes of stats. -h, --hour Print the last hour of stats. -d, --day Print the last day of stats. -t, --total Print total stats. -a, --all Print all stats. -r, --reset-stats Reset stats after printing them. -s, --sub-status Print subdevice status. -g, --gc Invoke GC before printing status. 这是正常 attach 后并有使用的状态, 可以看见 cache size, hits, cache mode 等详细信息\n[root@pure-test ~]# bcache-status -a --- bcache --- UUID 5da393d6-b157-4bb3-a105-235a1425894d Block Size 512 B Bucket Size 512.00 KiB Congested? False Read Congestion 2.0ms Write Congestion 20.0ms Total Cache Size 28 GiB Total Cache Used 6 GiB (23%) Total Cache Unused 22 GiB (77%) Evictable Cache 28 GiB (100%) Replacement Policy [lru] fifo random Cache Mode writethrough [writeback] writearound none Last 5min Hits 0 Last 5min Misses 0 Last 5min Bypass Hits 0 Last 5min Bypass Misses 0 Last 5min Bypassed 0 B Last Hour Hits 0 Last Hour Misses 0 Last Hour Bypass Hits 0 Last Hour Bypass Misses 0 Last Hour Bypassed 0 B Last Day Hits 10 (55%) Last Day Misses 8 Last Day Bypass Hits 0 Last Day Bypass Misses 0 Last Day Bypassed 0 B Total Hits 95 (53%) Total Misses 81 Total Bypass Hits 0 Total Bypass Misses 0 Total Bypassed 0 B 这是 detach 后的\n[root@pure-test ~]# bcache-status -a --- bcache --- UUID 5da393d6-b157-4bb3-a105-235a1425894d Block Size 512 B Bucket Size 512.00 KiB Congested? False Read Congestion 2.0ms Write Congestion 20.0ms Total Cache Size 28 GiB Total Cache Used 6 GiB (23%) Total Cache Unused 22 GiB (77%) Evictable Cache 28 GiB (100%) Replacement Policy [lru] fifo random Cache Mode (Various) Last 5min Hits 0 Last 5min Misses 0 Last 5min Bypass Hits 0 Last 5min Bypass Misses 0 Last 5min Bypassed 0 B Last Hour Hits 0 Last Hour Misses 0 Last Hour Bypass Hits 0 Last Hour Bypass Misses 0 Last Hour Bypassed 0 B Last Day Hits 10 (55%) Last Day Misses 8 Last Day Bypass Hits 0 Last Day Bypass Misses 0 Last Day Bypassed 0 B Total Hits 95 (53%) Total Misses 81 Total Bypass Hits 0 Total Bypass Misses 0 Total Bypassed 0 B 可以看见 Cache Mode (Various), 这表示 cache 设备已经 detach\n补充 # kernel doc 的描述: bcache.txt\narchlinux 的 wiki\n","date":"2018-01-09","externalUrl":null,"permalink":"/posts/2018/01/bcache-notes/","section":"博客","summary":"bcache 是 Linux 内核级别提供的一种缓存方案(要求内核版本 3.","title":"Bcache 小记","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" ","externalUrl":null,"permalink":"/about/","section":"呓语","summary":" ","title":"关于","type":"page"}]